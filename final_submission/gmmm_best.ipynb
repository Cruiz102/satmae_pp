{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb6fa69",
   "metadata": {},
   "source": [
    "# GMMM Submission AI Hackathon Lockeed Martin\n",
    "\n",
    " - Author: Cesar Ruiz, Edyan Cruz, Angel Morales, Yahid Diaz\n",
    " - Date: September 7 , 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a86a9",
   "metadata": {},
   "source": [
    "## Download dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86fca79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm==0.4.12 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (0.4.12)\n",
      "Requirement already satisfied: torch in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.8.0)\n",
      "Requirement already satisfied: opencv-python in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (4.12.0.88)\n",
      "Requirement already satisfied: matplotlib in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (3.10.6)\n",
      "Requirement already satisfied: numpy in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (2.2.6)\n",
      "Requirement already satisfied: pandas in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (2.3.2)\n",
      "Requirement already satisfied: seaborn in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (0.13.2)\n",
      "Requirement already satisfied: tqdm in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: torchvision in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (0.23.0)\n",
      "Requirement already satisfied: scikit-learn in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (1.7.1)\n",
      "Requirement already satisfied: kagglehub in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (0.3.13)\n",
      "Requirement already satisfied: huggingface_hub in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (0.34.4)\n",
      "Requirement already satisfied: albumentations in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (2.0.8)\n",
      "Requirement already satisfied: transformers in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (4.56.1)\n",
      "Requirement already satisfied: tensorboard in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (2.20.0)\n",
      "Requirement already satisfied: filelock in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 10)) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 10)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 10)) (3.6.0)\n",
      "Requirement already satisfied: pyyaml in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from kagglehub->-r requirements.txt (line 11)) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from kagglehub->-r requirements.txt (line 11)) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from huggingface_hub->-r requirements.txt (line 12)) (1.1.9)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 13)) (2.11.7)\n",
      "Requirement already satisfied: albucore==0.0.24 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 13)) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 13)) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 13)) (3.12.6)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 13)) (6.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 14)) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 14)) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 14)) (0.6.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (1.74.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (3.9)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (6.32.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (3.1.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 13)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 13)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 13)) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 15)) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from requests->kagglehub->-r requirements.txt (line 11)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from requests->kagglehub->-r requirements.txt (line 11)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from requests->kagglehub->-r requirements.txt (line 11)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ed/satmae_pp/.venv/lib/python3.12/site-packages (from requests->kagglehub->-r requirements.txt (line 11)) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eddfd75",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67af7341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"olebro/nasa-geographical-objects-multilabel-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4caed4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ed/.cache/kagglehub/datasets/olebro/nasa-geographical-objects-multilabel-dataset/versions/1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd29f4",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e3f878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 10:59:56.249425: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-07 10:59:56.386212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-07 10:59:58.547428: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import AutoImageProcessor,AutoModelForImageClassification,get_cosine_schedule_with_warmup\n",
    "import ast, numpy as np, pandas as pd, torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53cd27b",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6a90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_csv = \"data/train.csv\"\n",
    "val_csv = \"data/val.csv\"\n",
    "images_path = os.path.join(path, \"images\")\n",
    "image_size = 224\n",
    "model_id = \"facebook/dinov2-base\"\n",
    "batch_size = 32\n",
    "workers = 15\n",
    "num_classes = 10\n",
    "weight_decay = 0.05\n",
    "warmup_ratio = 0.05\n",
    "epochs = 3\n",
    "lr = 4e-5\n",
    "output_dir = \"output\"\n",
    "resume = False\n",
    "resume_path = \"/home/sagemaker-user/satmae_pp-1/final_submission/output/best_f1_0.9469.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940bc8f",
   "metadata": {},
   "source": [
    "## Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12aa0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleGeographicalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Reads `FileName` and `Label Vector`.\n",
    "    Builds a case-insensitive map of files and tolerates extension differences.\n",
    "    Returns (pixel_values, label_tensor).\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, images_dir, processor_name, image_size=224, augment=False):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "        self.augment_prob = 0.05\n",
    "\n",
    "        self.processor = AutoImageProcessor.from_pretrained(\n",
    "            processor_name,\n",
    "            do_resize=True,\n",
    "            size={\"height\": image_size, \"width\": image_size},\n",
    "            do_center_crop=False,\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _hflip(self, img):\n",
    "        return np.ascontiguousarray(img[:, ::-1, :])\n",
    "\n",
    "    def _vflip(self, img):\n",
    "        return np.ascontiguousarray(img[::-1, :, :])\n",
    "\n",
    "    def _rot90k(self, img, k):\n",
    "        if k % 4 == 0:\n",
    "            return img\n",
    "        return np.ascontiguousarray(np.rot90(img, k).copy())\n",
    "\n",
    "    def _small_rotate(self, img, max_deg=360):\n",
    "        ang = (np.random.rand() * 2 - 1) * max_deg\n",
    "        h, w = img.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w/2, h/2), ang, 1.0)\n",
    "        return cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
    "\n",
    "    def _random_resized_crop(self, img, scale=(0.6, 1.0), ratio=(0.75, 1.33)):\n",
    "        h, w = img.shape[:2]\n",
    "        area = h * w\n",
    "        for _ in range(10):\n",
    "            target_area = np.random.uniform(*scale) * area\n",
    "            log_ratio = (np.log(ratio[0]), np.log(ratio[1]))\n",
    "            aspect = np.exp(np.random.uniform(*log_ratio))\n",
    "            new_w = int(round(np.sqrt(target_area * aspect)))\n",
    "            new_h = int(round(np.sqrt(target_area / aspect)))\n",
    "            if 0 < new_w <= w and 0 < new_h <= h:\n",
    "                x1 = np.random.randint(0, w - new_w + 1)\n",
    "                y1 = np.random.randint(0, h - new_h + 1)\n",
    "                crop = img[y1:y1+new_h, x1:x1+new_w]\n",
    "                return cv2.resize(crop, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "        min_side = min(h, w)\n",
    "        y1 = (h - min_side) // 2; x1 = (w - min_side) // 2\n",
    "        crop = img[y1:y1+min_side, x1:x1+min_side]\n",
    "        return cv2.resize(crop, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    def _color_jitter(self, img, br=0.2, ct=0.2, sat=0.2):\n",
    "        img_f = img.astype(np.float32)\n",
    "        if br > 0:\n",
    "            factor = 1.0 + np.random.uniform(-br, br)\n",
    "            img_f = img_f * factor\n",
    "        if ct > 0:\n",
    "            mean = img_f.mean(axis=(0,1), keepdims=True)\n",
    "            factor = 1.0 + np.random.uniform(-ct, ct)\n",
    "            img_f = (img_f - mean) * factor + mean\n",
    "        img_f = np.clip(img_f, 0, 255)\n",
    "\n",
    "        if sat > 0:\n",
    "            hsv = cv2.cvtColor(img_f.astype(np.uint8), cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "            s_factor = 1.0 + np.random.uniform(-sat, sat)\n",
    "            hsv[...,1] = np.clip(hsv[...,1] * s_factor, 0, 255)\n",
    "            img_f = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB).astype(np.float32)\n",
    "\n",
    "        return np.clip(img_f, 0, 255).astype(np.uint8)\n",
    "\n",
    "    def _gaussian_blur(self, img):\n",
    "        k = np.random.choice([3, 5])\n",
    "        return cv2.GaussianBlur(img, (k, k), 0)\n",
    "  \n",
    "\n",
    "    def _gaussian_noise(self, img, sigma=5.0):\n",
    "        noise = np.random.randn(*img.shape).astype(np.float32) * sigma\n",
    "        out = img.astype(np.float32) + noise\n",
    "        return np.clip(out, 0, 255).astype(np.uint8)\n",
    "  \n",
    "\n",
    "    def _random_erasing(self, img, area_ratio=(0.02, 0.12), min_aspect=0.3):\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "        area = h * w\n",
    "        for _ in range(10):\n",
    "            target = np.random.uniform(*area_ratio) * area\n",
    "            aspect = np.random.uniform(min_aspect, 1/min_aspect)\n",
    "            er_w = int(round(np.sqrt(target * aspect)))\n",
    "            er_h = int(round(np.sqrt(target / aspect)))\n",
    "            if er_w < w and er_h < h:\n",
    "                x1 = np.random.randint(0, w - er_w + 1)\n",
    "                y1 = np.random.randint(0, h - er_h + 1)\n",
    "                fill = np.random.randint(0, 256, (er_h, er_w, 3), dtype=np.uint8)\n",
    "                img[y1:y1+er_h, x1:x1+er_w] = fill\n",
    "                return img\n",
    "        return img\n",
    "\n",
    "    def _maybe_augment(self, img_rgb):\n",
    "        if not self.augment:\n",
    "            return img_rgb\n",
    "\n",
    "        # Order of ops: geo -> crop -> color -> blur/noise -> erase\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._hflip(img_rgb)\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._vflip(img_rgb)\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._rot90k(img_rgb, np.random.randint(0, 4))\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._small_rotate(img_rgb, max_deg=90)\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._random_resized_crop(img_rgb, scale=(0.6, 1.0), ratio=(0.75, 1.33))\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._color_jitter(img_rgb, br=0.15, ct=0.15, sat=0.15)\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._gaussian_blur(img_rgb)\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._gaussian_noise(img_rgb, sigma=5.0)\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._random_erasing(img_rgb, area_ratio=(0.02, 0.12), min_aspect=0.3)\n",
    "\n",
    "        return img_rgb\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(images_path,self.df[\"FileName\"][idx])\n",
    "        image = cv2.imread(img_path)  # BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self._maybe_augment(image)\n",
    "        label_vec = ast.literal_eval(self.df[\"Label Vector\"][idx])\n",
    "        label = torch.tensor(label_vec, dtype=torch.float32)\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n",
    "        return pixel_values, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b9129",
   "metadata": {},
   "source": [
    "## Balance Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8836dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pos_weight_from_csv(train_csv_path: str, device: torch.device, eps: float = 1e-6):\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    Y = np.stack([np.array(ast.literal_eval(s), dtype=np.float32) for s in df[\"Label Vector\"]])  # (N, C)\n",
    "    pos = Y.sum(axis=0)                     # positives per class\n",
    "    neg = Y.shape[0] - pos                  # negatives per class\n",
    "    pw = neg / (pos + eps)                  # ratio -> larger weight for rare classes\n",
    "    # (Optional) clamp huge values if you have classes with 0 positives\n",
    "    pw = np.clip(pw, 1.0, 100.0)\n",
    "    return torch.tensor(pw, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5a613",
   "metadata": {},
   "source": [
    "## F1 Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea1ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_pred, all_true = [], []\n",
    "\n",
    "    for pixel_values, labels in dataloader:\n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(pixel_values=pixel_values).logits\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item() * pixel_values.size(0)\n",
    "\n",
    "        preds = (logits.sigmoid() > 0.5).int().cpu().numpy()\n",
    "        all_pred.append(preds)\n",
    "        all_true.append(labels.int().cpu().numpy())\n",
    "\n",
    "    all_pred = np.vstack(all_pred) if all_pred else np.zeros((0, 0))\n",
    "    all_true = np.vstack(all_true) if all_true else np.zeros((0, 0))\n",
    "\n",
    "    f1_micro = f1_score(all_true, all_pred, average=\"micro\", zero_division=0) if len(all_true) else 0.0\n",
    "    f1_macro = f1_score(all_true, all_pred, average=\"macro\", zero_division=0) if len(all_true) else 0.0\n",
    "    avg_loss = total_loss / max(len(dataloader.dataset), 1)\n",
    "\n",
    "    return {\"loss\": avg_loss, \"f1_micro\": f1_micro, \"f1_macro\": f1_macro, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ac9ec",
   "metadata": {},
   "source": [
    "## Training Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8acf985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, scheduler, device, scaler, criterion):\n",
    "    model.train()\n",
    "    seen = 0\n",
    "    running = 0.0\n",
    "    labeles =[]\n",
    "    for pixel_values, labels in dataloader:\n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        labeles.append(labels)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=torch.cuda.is_available()):\n",
    "            logits = model(pixel_values=pixel_values).logits\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running += loss.item() * pixel_values.size(0)\n",
    "        seen += pixel_values.size(0)\n",
    "\n",
    "    return labeles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ef37a",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe5f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73def4fba2ce4b69bbf3366fafaccfeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0f93d5e78249a5a8a0a342963f2add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Dinov2ForImageClassification were not initialized from the model checkpoint at facebook/dinov2-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 3 epochs\n"
     ]
    }
   ],
   "source": [
    "train_ds = KaggleGeographicalDataset(\n",
    "        train_csv, images_path, processor_name=model_id,\n",
    "        image_size=image_size, augment=True,\n",
    "    )\n",
    "val_ds = KaggleGeographicalDataset(\n",
    "        val_csv, images_path, processor_name=model_id,\n",
    "        image_size=image_size, augment=False\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True, drop_last=True\n",
    "    )\n",
    "val_loader = DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "        model_id,\n",
    "        num_labels=num_classes,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        ignore_mismatched_sizes=True,\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "param_groups = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0},\n",
    "    ]\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=lr)\n",
    "total_steps = max(1, len(train_loader)) * epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "pos_weight = compute_pos_weight_from_csv(train_csv, device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "# Logging mkdir intialization\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Train\n",
    "best_f1 = 0.0\n",
    "history_f1_val = []\n",
    "history_f1_train = []\n",
    "history_loss_val = []\n",
    "history_loss_train = []\n",
    "       # Store batch results\n",
    "actual_labels = []\n",
    "predicted_labels = []\n",
    "t0 = time.time()\n",
    "print(f\"Start training for {epochs} epochs\")\n",
    "for epoch in range(epochs):\n",
    "    actual_label = train_one_epoch(model, train_loader, optimizer, scheduler, device, scaler, criterion)\n",
    "    val_stats  = evaluate(model, val_loader, device, criterion)\n",
    "    train_stats = evaluate(model, train_loader, device, criterion)\n",
    "    print(f\"Epochs: {epoch}\\n\")\n",
    "    print(f\"[Eval] loss={val_stats['loss']:.4f} | f1_micro={val_stats['f1_micro']:.4f} | f1_macro={val_stats['f1_macro']:.4f}\")\n",
    "    print(f\"[Train] loss={train_stats['loss']:.4f} | f1_micro={train_stats['f1_micro']:.4f} | f1_macro={train_stats['f1_macro']:.4f}\\n\")\n",
    "    history_f1_val.append(val_stats['f1_micro'])\n",
    "    history_f1_train.append(train_stats['f1_micro'])\n",
    "    history_loss_val.append(val_stats[\"loss\"])\n",
    "    history_loss_train.append(train_stats[\"loss\"])\n",
    "\n",
    "    \n",
    "    actual_labels.append(actual_label)\n",
    "    predicted_labels.append(val_stats[\"logits\"])\n",
    "\n",
    "    if val_stats[\"f1_micro\"] > best_f1:\n",
    "        best_f1 = val_stats[\"f1_micro\"]\n",
    "        best_path = os.path.join(output_dir, f\"best_f1_{best_f1:.4f}.pt\")\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"pos_weight\": pos_weight.detach().cpu().tolist(), \n",
    "        }, best_path)\n",
    "\n",
    "        print(f\"New best F1_micro={best_f1:.4f} -> {best_path}\")\n",
    "total = str(datetime.timedelta(seconds=int(time.time() - t0)))\n",
    "print(f\"\\nDone. Best F1_micro={best_f1:.4f}. Total time: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e430cbaf",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41880143",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.arange(epochs) + 1\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_axis, history_loss_train, '-o', label = 'Train loss')\n",
    "ax.plot(x_axis, history_loss_val, '--<', label = 'Validation loss')\n",
    "ax.legend(fontsize=15)\n",
    "ax.grid(True)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x_axis, history_f1_train, '-o', label='Train F1')\n",
    "ax.plot(x_axis, history_f1_val, '--<', label='Validation F1')        \n",
    "ax.legend(fontsize=15)\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.set_ylabel('Epoch', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09524e6d",
   "metadata": {},
   "source": [
    "## Grade model from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5224b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device, criterion, incorrect_predictions):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_pred, all_true = [], []\n",
    "\n",
    "    for pixel_values, labels in dataloader:\n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(pixel_values=pixel_values).logits\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item() * pixel_values.size(0)\n",
    "\n",
    "        preds = (logits.sigmoid() > 0.5).int().cpu().numpy()\n",
    "        all_pred.append(preds)\n",
    "        all_true.append(labels.int().cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    all_pred = np.vstack(all_pred) if all_pred else np.zeros((0, 0))\n",
    "    all_true = np.vstack(all_true) if all_true else np.zeros((0, 0))\n",
    "\n",
    "    f1_micro = f1_score(all_true, all_pred, average=\"micro\", zero_division=0) if len(all_true) else 0.0\n",
    "    f1_macro = f1_score(all_true, all_pred, average=\"macro\", zero_division=0) if len(all_true) else 0.0\n",
    "    avg_loss = total_loss / max(len(dataloader.dataset), 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return {\"loss\": avg_loss, \"f1_micro\": f1_micro, \"f1_macro\": f1_macro}\n",
    "\n",
    "if resume:\n",
    "    ckpt = torch.load(resume_path, map_location=\"cpu\", weights_only=False)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "\n",
    "\n",
    "    if \"pos_weight\" in ckpt:\n",
    "        pos_weight = torch.tensor(ckpt[\"pos_weight\"], dtype=torch.float32, device=device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    stats = evaluate(model, val_loader, device, criterion, incorrect_predictions)\n",
    "    print(f\"[Eval] loss={stats['loss']:.4f} | f1_micro={stats['f1_micro']:.4f} | f1_macro={stats['f1_macro']:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89408b",
   "metadata": {},
   "source": [
    "## Grade Model (on validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "test_ds = KaggleGeographicalDataset(\"data/val.csv\", images_path, processor_name=model_id)\n",
    "test_dl = DataLoader(\n",
    "    test_ds, batch_size=batch_size, shuffle=False, num_workers=8, prefetch_factor=8,\n",
    "    pin_memory=True, persistent_workers=True\n",
    ")\n",
    "\n",
    "val_data_raw = pd.read_csv(val_csv)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "actual_labels = []\n",
    "predicted_labels = []\n",
    "image_paths = []  # To store paths of processed images\n",
    "incorrect_predictions = []  # To store (image_path, actual, predicted) for incorrect predictions\n",
    "\n",
    "f1_test = 0.0\n",
    "image_idx = 0  # Keep track of position in dataset\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for x, y in tqdm(test_dl):\n",
    "        bs = x.shape[0]               # ← local var; doesn’t clobber BATCH_SIZE\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(pixel_values=x).logits\n",
    "        loss = criterion(logits, y.float())\n",
    "        test_loss += loss.item()\n",
    "        pred = torch.sigmoid(logits)\n",
    "        # Convert to numpy for easier handling\n",
    "        y_np = to_numpy(y)\n",
    "        pred_np = to_numpy((pred > 0.5).int())\n",
    "\n",
    "        # Calculate F1 score for this batch\n",
    "        is_correct = f1_score(y_np, pred_np, average='micro')\n",
    "        f1_test += is_correct\n",
    "\n",
    "        # Store batch results\n",
    "        actual_labels.append(y_np)\n",
    "        predicted_labels.append(pred_np)\n",
    "\n",
    "        # Check each image in the batch for correctness\n",
    "        for i in range(bs):\n",
    "            # Get the current image's index in the full dataset\n",
    "            curr_idx = image_idx + i\n",
    "            if curr_idx < len(test_ds):  # Ensure we don't go out of bounds\n",
    "                # Get image path from test_data_raw\n",
    "                img_path = val_data_raw.iloc[curr_idx]['FileName']\n",
    "                \n",
    "                # Compare prediction with actual label\n",
    "                if not np.array_equal(pred_np[i], y_np[i]):\n",
    "                    # This is an incorrect prediction\n",
    "                    incorrect_predictions.append({\n",
    "                        'image_path': img_path,\n",
    "                        'actual': y_np[i],\n",
    "                        'predicted': pred_np[i]\n",
    "                    })\n",
    "\n",
    "        # Update image index for next batch\n",
    "        image_idx += bs\n",
    "\n",
    "    f1_test /= np.ceil(len(test_dl.dataset) / bs)\n",
    "\n",
    "print(f'Validation F1: {f1_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dbc6af",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multilabel_confusion_matrix(y_true_arrays: List[list], y_pred_arrays: list, test_data_raw: pd.DataFrame):\n",
    "    valid_combinations = []\n",
    "    valid_label_names = {}\n",
    "    \n",
    "    # unique label vectors from test data\n",
    "    for _, row in test_data_raw.drop_duplicates(subset=['Label Vector']).iterrows():\n",
    "        label_vector = np.array(row['Label Vector'].strip('[]').split(', '), dtype=int)\n",
    "        if isinstance(label_vector, np.ndarray):\n",
    "            vector_tuple = tuple(label_vector.flatten())\n",
    "        else:\n",
    "            for arr in label_vector:\n",
    "                if isinstance(arr, np.ndarray):\n",
    "                    vector_tuple = tuple(arr.flatten())\n",
    "                    break\n",
    "                    \n",
    "        valid_combinations.append(vector_tuple)\n",
    "        valid_label_names[vector_tuple] = row['Label String']\n",
    "    \n",
    "    # add \"Other\" category\n",
    "    other_idx = len(valid_combinations)\n",
    "    \n",
    "    # convert true and predicted arrays to tuples\n",
    "    y_true_tuples = []\n",
    "    y_pred_tuples = []\n",
    "    \n",
    "    for array in y_true_arrays:\n",
    "        for row in array:\n",
    "            y_true_tuples.append(tuple(row))\n",
    "            \n",
    "    for array in y_pred_arrays:\n",
    "        for row in array:\n",
    "            y_pred_tuples.append(tuple(row))\n",
    "    \n",
    "    # Map each tuple to the index of its class or to \"Other\"\n",
    "    def get_class_index(tup):\n",
    "        if tup in valid_combinations:\n",
    "            return valid_combinations.index(tup)\n",
    "        else:\n",
    "            return other_idx\n",
    "    \n",
    "    # convert tuples to class indices\n",
    "    y_true_indices = [get_class_index(t) for t in y_true_tuples]\n",
    "    y_pred_indices = [get_class_index(t) for t in y_pred_tuples]\n",
    "    \n",
    "    # create labels for the confusion matrix\n",
    "    labels = [valid_label_names[combo] for combo in valid_combinations] + [\"Other\"]\n",
    "    \n",
    "    # create the confusion matrix\n",
    "    cm = confusion_matrix(y_true_indices, y_pred_indices, \n",
    "                          labels=range(len(valid_combinations) + 1))\n",
    "    \n",
    "    # create a DataFrame for better visualization\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    \n",
    "    return cm, cm_df, labels\n",
    "\n",
    "def plot_confusion_matrix(cm_df, labels, figsize=(15, 15)):\n",
    "    # create the plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix for Multilabel Classification')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "val_df = pd.read_csv(val_csv)\n",
    "cm, cm_df, labels = create_multilabel_confusion_matrix(\n",
    "    actual_labels, predicted_labels,val_df )\n",
    "fig = plot_confusion_matrix(cm_df, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf4edc2",
   "metadata": {},
   "source": [
    "## Where the model is underperforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a54552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the incorrect images\n",
    "def plot_incorrect_predictions(incorrect_preds, label_names=None, max_images=20):\n",
    "    \"\"\"\n",
    "    Plot images with incorrect predictions\n",
    "    \n",
    "    Parameters:\n",
    "    - incorrect_preds: List of dicts with keys 'image_path', 'actual', 'predicted'\n",
    "    - label_names: List of label names corresponding to positions in one-hot vector\n",
    "    - max_images: Maximum number of images to plot\n",
    "    \"\"\"\n",
    "    # Limit number of images to display\n",
    "    num_to_show = min(max_images, len(incorrect_preds))\n",
    "    \n",
    "    # Calculate grid size\n",
    "    grid_size = int(np.ceil(np.sqrt(num_to_show)))\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    \n",
    "    for i in range(num_to_show):\n",
    "        plt.subplot(grid_size, grid_size, i + 1)\n",
    "        \n",
    "        # Load and display image\n",
    "        img_path = os.path.join(images_path, incorrect_preds[i]['image_path'])\n",
    "        img = plt.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        # Format actual and predicted labels\n",
    "        actual = incorrect_preds[i]['actual']\n",
    "        predicted = incorrect_preds[i]['predicted']\n",
    "        \n",
    "        if label_names:\n",
    "            # Convert one-hot encoded vectors to label names\n",
    "            actual_labels_text = ', '.join([label_names[j] for j, val in enumerate(actual) if val == 1])\n",
    "            pred_labels_text = ', '.join([label_names[j] for j, val in enumerate(predicted) if val == 1])\n",
    "            if not actual_labels_text:\n",
    "                actual_labels_text = \"None\"\n",
    "            if not pred_labels_text:\n",
    "                pred_labels_text = \"None\"\n",
    "        else:\n",
    "            # Display raw vectors\n",
    "            actual_indices = [j for j, val in enumerate(actual) if val == 1]\n",
    "            pred_indices = [j for j, val in enumerate(predicted) if val == 1]\n",
    "            actual_labels_text = f\"[{', '.join(map(str, actual_indices))}]\"\n",
    "            pred_labels_text = f\"[{', '.join(map(str, pred_indices))}]\"\n",
    "        \n",
    "        # set title\n",
    "        plt.title(f\"Actual: {actual_labels_text}\\nPredicted: {pred_labels_text}\", fontsize=10)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot incorrect predictions\n",
    "plot_incorrect_predictions(incorrect_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b2146e",
   "metadata": {},
   "source": [
    "## Model Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e3393",
   "metadata": {},
   "source": [
    "### Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ecd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model params: {n_params/1e6:.2f}M\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model parameters: {total_params/1e6:.2f}M total\")\n",
    "print(f\"Trainable parameters: {trainable_params/1e6:.2f}M\")\n",
    "\n",
    "# Show the architecture\n",
    "print(\"\\nModel architecture:\\n\")\n",
    "print(model)\n",
    "\n",
    "# Optional: per-layer parameter counts\n",
    "print(\"\\nParameter breakdown per layer:\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name:<60} {param.numel()/1e6:.3f}M\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
