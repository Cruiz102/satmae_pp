{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb6fa69",
   "metadata": {},
   "source": [
    "# GMMM Submission AI Hackathon Lockeed Martin\n",
    "\n",
    " - Author: Cesar Ruiz, Edyan Cruz, Angel Morales, Yahid Diaz\n",
    " - Date: September 7 , 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a86a9",
   "metadata": {},
   "source": [
    "## Download dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fca79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eddfd75",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af7341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"olebro/nasa-geographical-objects-multilabel-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caed4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd29f4",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e3f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import AutoImageProcessor,AutoModelForImageClassification,get_cosine_schedule_with_warmup\n",
    "import ast, numpy as np, pandas as pd, torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53cd27b",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_csv = \"data/train.csv\"\n",
    "val_csv = \"data/val.csv\"\n",
    "images_path = os.path.join(path, \"images\")\n",
    "image_size = 224\n",
    "model_id = \"facebook/dinov2-base\"\n",
    "batch_size = 32\n",
    "workers = 15\n",
    "num_classes = 10\n",
    "weight_decay = 0.05\n",
    "warmup_ratio = 0.05\n",
    "epochs = 3\n",
    "lr = 4e-5\n",
    "output_dir = \"output\"\n",
    "resume = False\n",
    "resume_path = \"/home/sagemaker-user/satmae_pp-1/final_submission/output/best_f1_0.9469.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940bc8f",
   "metadata": {},
   "source": [
    "## Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleGeographicalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Reads `FileName` and `Label Vector`.\n",
    "    Builds a case-insensitive map of files and tolerates extension differences.\n",
    "    Returns (pixel_values, label_tensor).\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, images_dir, processor_name, image_size=224, augment=False):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "        self.augment_prob = 0.05\n",
    "\n",
    "        self.processor = AutoImageProcessor.from_pretrained(\n",
    "            processor_name,\n",
    "            do_resize=True,\n",
    "            size={\"height\": image_size, \"width\": image_size},\n",
    "            do_center_crop=False,\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _hflip(self, img):\n",
    "        return np.ascontiguousarray(img[:, ::-1, :])\n",
    "\n",
    "    def _vflip(self, img):\n",
    "        return np.ascontiguousarray(img[::-1, :, :])\n",
    "\n",
    "    def _rot90k(self, img, k):\n",
    "        if k % 4 == 0:\n",
    "            return img\n",
    "        return np.ascontiguousarray(np.rot90(img, k).copy())\n",
    "\n",
    "    def _small_rotate(self, img, max_deg=360):\n",
    "        ang = (np.random.rand() * 2 - 1) * max_deg\n",
    "        h, w = img.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w/2, h/2), ang, 1.0)\n",
    "        return cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
    "\n",
    "    def _random_resized_crop(self, img, scale=(0.6, 1.0), ratio=(0.75, 1.33)):\n",
    "        h, w = img.shape[:2]\n",
    "        area = h * w\n",
    "        for _ in range(10):\n",
    "            target_area = np.random.uniform(*scale) * area\n",
    "            log_ratio = (np.log(ratio[0]), np.log(ratio[1]))\n",
    "            aspect = np.exp(np.random.uniform(*log_ratio))\n",
    "            new_w = int(round(np.sqrt(target_area * aspect)))\n",
    "            new_h = int(round(np.sqrt(target_area / aspect)))\n",
    "            if 0 < new_w <= w and 0 < new_h <= h:\n",
    "                x1 = np.random.randint(0, w - new_w + 1)\n",
    "                y1 = np.random.randint(0, h - new_h + 1)\n",
    "                crop = img[y1:y1+new_h, x1:x1+new_w]\n",
    "                return cv2.resize(crop, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "        min_side = min(h, w)\n",
    "        y1 = (h - min_side) // 2; x1 = (w - min_side) // 2\n",
    "        crop = img[y1:y1+min_side, x1:x1+min_side]\n",
    "        return cv2.resize(crop, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    def _color_jitter(self, img, br=0.2, ct=0.2, sat=0.2):\n",
    "        img_f = img.astype(np.float32)\n",
    "        if br > 0:\n",
    "            factor = 1.0 + np.random.uniform(-br, br)\n",
    "            img_f = img_f * factor\n",
    "        if ct > 0:\n",
    "            mean = img_f.mean(axis=(0,1), keepdims=True)\n",
    "            factor = 1.0 + np.random.uniform(-ct, ct)\n",
    "            img_f = (img_f - mean) * factor + mean\n",
    "        img_f = np.clip(img_f, 0, 255)\n",
    "\n",
    "        if sat > 0:\n",
    "            hsv = cv2.cvtColor(img_f.astype(np.uint8), cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "            s_factor = 1.0 + np.random.uniform(-sat, sat)\n",
    "            hsv[...,1] = np.clip(hsv[...,1] * s_factor, 0, 255)\n",
    "            img_f = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB).astype(np.float32)\n",
    "\n",
    "        return np.clip(img_f, 0, 255).astype(np.uint8)\n",
    "\n",
    "    def _gaussian_blur(self, img):\n",
    "        k = np.random.choice([3, 5])\n",
    "        return cv2.GaussianBlur(img, (k, k), 0)\n",
    "  \n",
    "\n",
    "    def _gaussian_noise(self, img, sigma=5.0):\n",
    "        noise = np.random.randn(*img.shape).astype(np.float32) * sigma\n",
    "        out = img.astype(np.float32) + noise\n",
    "        return np.clip(out, 0, 255).astype(np.uint8)\n",
    "  \n",
    "\n",
    "    def _random_erasing(self, img, area_ratio=(0.02, 0.12), min_aspect=0.3):\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "        area = h * w\n",
    "        for _ in range(10):\n",
    "            target = np.random.uniform(*area_ratio) * area\n",
    "            aspect = np.random.uniform(min_aspect, 1/min_aspect)\n",
    "            er_w = int(round(np.sqrt(target * aspect)))\n",
    "            er_h = int(round(np.sqrt(target / aspect)))\n",
    "            if er_w < w and er_h < h:\n",
    "                x1 = np.random.randint(0, w - er_w + 1)\n",
    "                y1 = np.random.randint(0, h - er_h + 1)\n",
    "                fill = np.random.randint(0, 256, (er_h, er_w, 3), dtype=np.uint8)\n",
    "                img[y1:y1+er_h, x1:x1+er_w] = fill\n",
    "                return img\n",
    "        return img\n",
    "\n",
    "    def _maybe_augment(self, img_rgb):\n",
    "        if not self.augment:\n",
    "            return img_rgb\n",
    "\n",
    "        # Order of ops: geo -> crop -> color -> blur/noise -> erase\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._hflip(img_rgb)\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._vflip(img_rgb)\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._rot90k(img_rgb, np.random.randint(0, 4))\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._small_rotate(img_rgb, max_deg=90)\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._random_resized_crop(img_rgb, scale=(0.6, 1.0), ratio=(0.75, 1.33))\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._color_jitter(img_rgb, br=0.15, ct=0.15, sat=0.15)\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._gaussian_blur(img_rgb)\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._gaussian_noise(img_rgb, sigma=5.0)\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            img_rgb = self._random_erasing(img_rgb, area_ratio=(0.02, 0.12), min_aspect=0.3)\n",
    "\n",
    "        return img_rgb\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(images_path,self.df[\"FileName\"][idx])\n",
    "        image = cv2.imread(img_path)  # BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self._maybe_augment(image)\n",
    "        label_vec = ast.literal_eval(self.df[\"Label Vector\"][idx])\n",
    "        label = torch.tensor(label_vec, dtype=torch.float32)\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n",
    "        return pixel_values, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b9129",
   "metadata": {},
   "source": [
    "## Balance Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8836dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pos_weight_from_csv(train_csv_path: str, device: torch.device, eps: float = 1e-6):\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    Y = np.stack([np.array(ast.literal_eval(s), dtype=np.float32) for s in df[\"Label Vector\"]])  # (N, C)\n",
    "    pos = Y.sum(axis=0)                     # positives per class\n",
    "    neg = Y.shape[0] - pos                  # negatives per class\n",
    "    pw = neg / (pos + eps)                  # ratio -> larger weight for rare classes\n",
    "    # (Optional) clamp huge values if you have classes with 0 positives\n",
    "    pw = np.clip(pw, 1.0, 100.0)\n",
    "    return torch.tensor(pw, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5a613",
   "metadata": {},
   "source": [
    "## F1 Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea1ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_pred, all_true = [], []\n",
    "\n",
    "    for pixel_values, labels in dataloader:\n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(pixel_values=pixel_values).logits\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item() * pixel_values.size(0)\n",
    "\n",
    "        preds = (logits.sigmoid() > 0.5).int().cpu().numpy()\n",
    "        all_pred.append(preds)\n",
    "        all_true.append(labels.int().cpu().numpy())\n",
    "\n",
    "    all_pred = np.vstack(all_pred) if all_pred else np.zeros((0, 0))\n",
    "    all_true = np.vstack(all_true) if all_true else np.zeros((0, 0))\n",
    "\n",
    "    f1_micro = f1_score(all_true, all_pred, average=\"micro\", zero_division=0) if len(all_true) else 0.0\n",
    "    f1_macro = f1_score(all_true, all_pred, average=\"macro\", zero_division=0) if len(all_true) else 0.0\n",
    "    avg_loss = total_loss / max(len(dataloader.dataset), 1)\n",
    "\n",
    "    return {\"loss\": avg_loss, \"f1_micro\": f1_micro, \"f1_macro\": f1_macro, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ac9ec",
   "metadata": {},
   "source": [
    "## Training Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, scheduler, device, scaler, criterion):\n",
    "    model.train()\n",
    "    seen = 0\n",
    "    running = 0.0\n",
    "    labeles =[]\n",
    "    for pixel_values, labels in dataloader:\n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        labeles.append(labels)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=torch.cuda.is_available()):\n",
    "            logits = model(pixel_values=pixel_values).logits\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running += loss.item() * pixel_values.size(0)\n",
    "        seen += pixel_values.size(0)\n",
    "\n",
    "    return labeles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ef37a",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = KaggleGeographicalDataset(\n",
    "        train_csv, images_path, processor_name=model_id,\n",
    "        image_size=image_size, augment=True,\n",
    "    )\n",
    "val_ds = KaggleGeographicalDataset(\n",
    "        val_csv, images_path, processor_name=model_id,\n",
    "        image_size=image_size, augment=False\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True, drop_last=True\n",
    "    )\n",
    "val_loader = DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "        model_id,\n",
    "        num_labels=num_classes,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        ignore_mismatched_sizes=True,\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "param_groups = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0},\n",
    "    ]\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=lr)\n",
    "total_steps = max(1, len(train_loader)) * epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "pos_weight = compute_pos_weight_from_csv(train_csv, device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "# Logging mkdir intialization\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Train\n",
    "best_f1 = 0.0\n",
    "history_f1_val = []\n",
    "history_f1_train = []\n",
    "history_loss_val = []\n",
    "history_loss_train = []\n",
    "       # Store batch results\n",
    "actual_labels = []\n",
    "predicted_labels = []\n",
    "t0 = time.time()\n",
    "print(f\"Start training for {epochs} epochs\")\n",
    "for epoch in range(epochs):\n",
    "    actual_label = train_one_epoch(model, train_loader, optimizer, scheduler, device, scaler, criterion)\n",
    "    val_stats  = evaluate(model, val_loader, device, criterion)\n",
    "    train_stats = evaluate(model, train_loader, device, criterion)\n",
    "    print(f\"Epochs: {epoch}\\n\")\n",
    "    print(f\"[Eval] loss={val_stats['loss']:.4f} | f1_micro={val_stats['f1_micro']:.4f} | f1_macro={val_stats['f1_macro']:.4f}\")\n",
    "    print(f\"[Train] loss={train_stats['loss']:.4f} | f1_micro={train_stats['f1_micro']:.4f} | f1_macro={train_stats['f1_macro']:.4f}\\n\")\n",
    "    history_f1_val.append(val_stats['f1_micro'])\n",
    "    history_f1_train.append(train_stats['f1_micro'])\n",
    "    history_loss_val.append(val_stats[\"loss\"])\n",
    "    history_loss_train.append(train_stats[\"loss\"])\n",
    "\n",
    "    \n",
    "    actual_labels.append(actual_label)\n",
    "    predicted_labels.append(val_stats[\"logits\"])\n",
    "\n",
    "    if val_stats[\"f1_micro\"] > best_f1:\n",
    "        best_f1 = val_stats[\"f1_micro\"]\n",
    "        best_path = os.path.join(output_dir, f\"best_f1_{best_f1:.4f}.pt\")\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"pos_weight\": pos_weight.detach().cpu().tolist(), \n",
    "        }, best_path)\n",
    "\n",
    "        print(f\"New best F1_micro={best_f1:.4f} -> {best_path}\")\n",
    "total = str(datetime.timedelta(seconds=int(time.time() - t0)))\n",
    "print(f\"\\nDone. Best F1_micro={best_f1:.4f}. Total time: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e430cbaf",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41880143",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.arange(epochs) + 1\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_axis, history_loss_train, '-o', label = 'Train loss')\n",
    "ax.plot(x_axis, history_loss_val, '--<', label = 'Validation loss')\n",
    "ax.legend(fontsize=15)\n",
    "ax.grid(True)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x_axis, history_f1_train, '-o', label='Train F1')\n",
    "ax.plot(x_axis, history_f1_val, '--<', label='Validation F1')        \n",
    "ax.legend(fontsize=15)\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.set_ylabel('Epoch', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09524e6d",
   "metadata": {},
   "source": [
    "## Grade model from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5224b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device, criterion, incorrect_predictions):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_pred, all_true = [], []\n",
    "\n",
    "    for pixel_values, labels in dataloader:\n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(pixel_values=pixel_values).logits\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item() * pixel_values.size(0)\n",
    "\n",
    "        preds = (logits.sigmoid() > 0.5).int().cpu().numpy()\n",
    "        all_pred.append(preds)\n",
    "        all_true.append(labels.int().cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    all_pred = np.vstack(all_pred) if all_pred else np.zeros((0, 0))\n",
    "    all_true = np.vstack(all_true) if all_true else np.zeros((0, 0))\n",
    "\n",
    "    f1_micro = f1_score(all_true, all_pred, average=\"micro\", zero_division=0) if len(all_true) else 0.0\n",
    "    f1_macro = f1_score(all_true, all_pred, average=\"macro\", zero_division=0) if len(all_true) else 0.0\n",
    "    avg_loss = total_loss / max(len(dataloader.dataset), 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return {\"loss\": avg_loss, \"f1_micro\": f1_micro, \"f1_macro\": f1_macro}\n",
    "\n",
    "if resume:\n",
    "    ckpt = torch.load(resume_path, map_location=\"cpu\", weights_only=False)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "\n",
    "\n",
    "    if \"pos_weight\" in ckpt:\n",
    "        pos_weight = torch.tensor(ckpt[\"pos_weight\"], dtype=torch.float32, device=device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    stats = evaluate(model, val_loader, device, criterion, incorrect_predictions)\n",
    "    print(f\"[Eval] loss={stats['loss']:.4f} | f1_micro={stats['f1_micro']:.4f} | f1_macro={stats['f1_macro']:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89408b",
   "metadata": {},
   "source": [
    "## Grade Model (on validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "test_ds = KaggleGeographicalDataset(\"data/val.csv\", images_path, processor_name=model_id)\n",
    "test_dl = DataLoader(\n",
    "    test_ds, batch_size=batch_size, shuffle=False, num_workers=8, prefetch_factor=8,\n",
    "    pin_memory=True, persistent_workers=True\n",
    ")\n",
    "\n",
    "val_data_raw = pd.read_csv(val_csv)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "actual_labels = []\n",
    "predicted_labels = []\n",
    "image_paths = []  # To store paths of processed images\n",
    "incorrect_predictions = []  # To store (image_path, actual, predicted) for incorrect predictions\n",
    "\n",
    "f1_test = 0.0\n",
    "image_idx = 0  # Keep track of position in dataset\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for x, y in tqdm(test_dl):\n",
    "        bs = batch_size\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(pixel_values=x).logits\n",
    "        loss = criterion(logits, y.float())\n",
    "        test_loss += loss.item()\n",
    "        pred = torch.sigmoid(logits)\n",
    "        # Convert to numpy for easier handling\n",
    "        y_np = to_numpy(y)\n",
    "        pred_np = to_numpy((pred > 0.5).int())\n",
    "\n",
    "        # Calculate F1 score for this batch\n",
    "        is_correct = f1_score(y_np, pred_np, average='micro')\n",
    "        f1_test += is_correct\n",
    "\n",
    "        # Store batch results\n",
    "        actual_labels.append(y_np)\n",
    "        predicted_labels.append(pred_np)\n",
    "\n",
    "        # Check each image in the batch for correctness\n",
    "        for i in range(bs):\n",
    "            # Get the current image's index in the full dataset\n",
    "            curr_idx = image_idx + i\n",
    "            if curr_idx < len(test_ds):  # Ensure we don't go out of bounds\n",
    "                # Get image path from test_data_raw\n",
    "                img_path = val_data_raw.iloc[curr_idx]['FileName']\n",
    "                \n",
    "                # Compare prediction with actual label\n",
    "                if not np.array_equal(pred_np[i], y_np[i]):\n",
    "                    # This is an incorrect prediction\n",
    "                    incorrect_predictions.append({\n",
    "                        'image_path': img_path,\n",
    "                        'actual': y_np[i],\n",
    "                        'predicted': pred_np[i]\n",
    "                    })\n",
    "\n",
    "        # Update image index for next batch\n",
    "        image_idx += bs\n",
    "\n",
    "    f1_test /= np.ceil(len(test_dl.dataset) / bs)\n",
    "\n",
    "print(f'Validation F1: {f1_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dbc6af",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multilabel_confusion_matrix(y_true_arrays: List[list], y_pred_arrays: list, test_data_raw: pd.DataFrame):\n",
    "    valid_combinations = []\n",
    "    valid_label_names = {}\n",
    "    \n",
    "    # unique label vectors from test data\n",
    "    for _, row in test_data_raw.drop_duplicates(subset=['Label Vector']).iterrows():\n",
    "        label_vector = np.array(row['Label Vector'].strip('[]').split(', '), dtype=int)\n",
    "        if isinstance(label_vector, np.ndarray):\n",
    "            vector_tuple = tuple(label_vector.flatten())\n",
    "        else:\n",
    "            for arr in label_vector:\n",
    "                if isinstance(arr, np.ndarray):\n",
    "                    vector_tuple = tuple(arr.flatten())\n",
    "                    break\n",
    "                    \n",
    "        valid_combinations.append(vector_tuple)\n",
    "        valid_label_names[vector_tuple] = row['Label String']\n",
    "    \n",
    "    # add \"Other\" category\n",
    "    other_idx = len(valid_combinations)\n",
    "    \n",
    "    # convert true and predicted arrays to tuples\n",
    "    y_true_tuples = []\n",
    "    y_pred_tuples = []\n",
    "    \n",
    "    for array in y_true_arrays:\n",
    "        for row in array:\n",
    "            y_true_tuples.append(tuple(row))\n",
    "            \n",
    "    for array in y_pred_arrays:\n",
    "        for row in array:\n",
    "            y_pred_tuples.append(tuple(row))\n",
    "    \n",
    "    # Map each tuple to the index of its class or to \"Other\"\n",
    "    def get_class_index(tup):\n",
    "        if tup in valid_combinations:\n",
    "            return valid_combinations.index(tup)\n",
    "        else:\n",
    "            return other_idx\n",
    "    \n",
    "    # convert tuples to class indices\n",
    "    y_true_indices = [get_class_index(t) for t in y_true_tuples]\n",
    "    y_pred_indices = [get_class_index(t) for t in y_pred_tuples]\n",
    "    \n",
    "    # create labels for the confusion matrix\n",
    "    labels = [valid_label_names[combo] for combo in valid_combinations] + [\"Other\"]\n",
    "    \n",
    "    # create the confusion matrix\n",
    "    cm = confusion_matrix(y_true_indices, y_pred_indices, \n",
    "                          labels=range(len(valid_combinations) + 1))\n",
    "    \n",
    "    # create a DataFrame for better visualization\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    \n",
    "    return cm, cm_df, labels\n",
    "\n",
    "def plot_confusion_matrix(cm_df, labels, figsize=(15, 15)):\n",
    "    # create the plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix for Multilabel Classification')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "val_df = pd.read_csv(val_csv)\n",
    "cm, cm_df, labels = create_multilabel_confusion_matrix(\n",
    "    actual_labels, predicted_labels,val_df )\n",
    "fig = plot_confusion_matrix(cm_df, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf4edc2",
   "metadata": {},
   "source": [
    "## Where the model is underperforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a54552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the incorrect images\n",
    "def plot_incorrect_predictions(incorrect_preds, label_names=None, max_images=20):\n",
    "    \"\"\"\n",
    "    Plot images with incorrect predictions\n",
    "    \n",
    "    Parameters:\n",
    "    - incorrect_preds: List of dicts with keys 'image_path', 'actual', 'predicted'\n",
    "    - label_names: List of label names corresponding to positions in one-hot vector\n",
    "    - max_images: Maximum number of images to plot\n",
    "    \"\"\"\n",
    "    # Limit number of images to display\n",
    "    num_to_show = min(max_images, len(incorrect_preds))\n",
    "    \n",
    "    # Calculate grid size\n",
    "    grid_size = int(np.ceil(np.sqrt(num_to_show)))\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    \n",
    "    for i in range(num_to_show):\n",
    "        plt.subplot(grid_size, grid_size, i + 1)\n",
    "        \n",
    "        # Load and display image\n",
    "        img_path = os.path.join(images_path, incorrect_preds[i]['image_path'])\n",
    "        img = plt.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        # Format actual and predicted labels\n",
    "        actual = incorrect_preds[i]['actual']\n",
    "        predicted = incorrect_preds[i]['predicted']\n",
    "        \n",
    "        if label_names:\n",
    "            # Convert one-hot encoded vectors to label names\n",
    "            actual_labels_text = ', '.join([label_names[j] for j, val in enumerate(actual) if val == 1])\n",
    "            pred_labels_text = ', '.join([label_names[j] for j, val in enumerate(predicted) if val == 1])\n",
    "            if not actual_labels_text:\n",
    "                actual_labels_text = \"None\"\n",
    "            if not pred_labels_text:\n",
    "                pred_labels_text = \"None\"\n",
    "        else:\n",
    "            # Display raw vectors\n",
    "            actual_indices = [j for j, val in enumerate(actual) if val == 1]\n",
    "            pred_indices = [j for j, val in enumerate(predicted) if val == 1]\n",
    "            actual_labels_text = f\"[{', '.join(map(str, actual_indices))}]\"\n",
    "            pred_labels_text = f\"[{', '.join(map(str, pred_indices))}]\"\n",
    "        \n",
    "        # set title\n",
    "        plt.title(f\"Actual: {actual_labels_text}\\nPredicted: {pred_labels_text}\", fontsize=10)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot incorrect predictions\n",
    "plot_incorrect_predictions(incorrect_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b2146e",
   "metadata": {},
   "source": [
    "## Model Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e3393",
   "metadata": {},
   "source": [
    "### Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ecd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model params: {n_params/1e6:.2f}M\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model parameters: {total_params/1e6:.2f}M total\")\n",
    "print(f\"Trainable parameters: {trainable_params/1e6:.2f}M\")\n",
    "\n",
    "# Show the architecture\n",
    "print(\"\\nModel architecture:\\n\")\n",
    "print(model)\n",
    "\n",
    "# Optional: per-layer parameter counts\n",
    "print(\"\\nParameter breakdown per layer:\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name:<60} {param.numel()/1e6:.3f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba30c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List, Tuple, Union\n",
    "\n",
    "def _extract_pixel_values(batch, device):\n",
    "    pv = None\n",
    "    if isinstance(batch, dict):\n",
    "        for k in (\"pixel_values\", \"images\", \"inputs\"):\n",
    "            if k in batch:\n",
    "                pv = batch[k]; break\n",
    "        if pv is None:\n",
    "            raise KeyError(\"Batch dict missing 'pixel_values'/'images'/'inputs'.\")\n",
    "    elif isinstance(batch, (list, tuple)):\n",
    "        if len(batch) > 0 and isinstance(batch[0], dict):\n",
    "            if \"pixel_values\" in batch[0]:\n",
    "                pv = torch.stack([b[\"pixel_values\"] for b in batch], dim=0)\n",
    "            elif \"images\" in batch[0]:\n",
    "                pv = torch.stack([b[\"images\"] for b in batch], dim=0)\n",
    "            else:\n",
    "                raise KeyError(\"List of dicts lacks 'pixel_values'/'images'.\")\n",
    "        else:\n",
    "            pv = batch[0]\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported batch type: {type(batch)}\")\n",
    "\n",
    "    if pv.ndim == 3:\n",
    "        pv = pv.unsqueeze(0)\n",
    "    elif pv.ndim != 4:\n",
    "        raise ValueError(f\"Unexpected pixel_values shape: {tuple(pv.shape)}\")\n",
    "    if pv.dtype == torch.uint8:\n",
    "        pv = pv.float() / 255.0\n",
    "    return pv.to(device)\n",
    "\n",
    "\n",
    "def _denorm_rgb(pixel_values: torch.Tensor,\n",
    "                mean: List[float] = [0.485, 0.456, 0.406],\n",
    "                std:  List[float] = [0.229, 0.224, 0.225]) -> List:\n",
    "    mean_t = torch.tensor(mean, device=pixel_values.device).view(1,3,1,1)\n",
    "    std_t  = torch.tensor(std,  device=pixel_values.device).view(1,3,1,1)\n",
    "    rgb = (pixel_values * std_t + mean_t).clamp(0,1).detach().cpu()\n",
    "    return [rgb[i].permute(1,2,0).numpy() for i in range(rgb.shape[0])]\n",
    "\n",
    "\n",
    "def _infer_grid_hw_from_tokens(T_no_cls: int) -> Tuple[int,int]:\n",
    "    s = int(round(math.sqrt(T_no_cls)))\n",
    "    if s * s == T_no_cls:\n",
    "        return s, s\n",
    "    best = (1, T_no_cls); best_diff = T_no_cls - 1\n",
    "    for h in range(1, int(math.sqrt(T_no_cls)) + 1):\n",
    "        if T_no_cls % h == 0:\n",
    "            w = T_no_cls // h\n",
    "            if abs(h - w) < best_diff:\n",
    "                best_diff = abs(h - w); best = (h, w)\n",
    "    return best\n",
    "\n",
    "\n",
    "def _grid_hw_from_model_and_inputs(model, pixel_values, tokens_no_cls: int) -> Tuple[int,int]:\n",
    "    cfg = getattr(model, \"config\", None)\n",
    "    ph = pw = None\n",
    "    image_size = getattr(cfg, \"image_size\", None) if cfg else None\n",
    "    patch_size = getattr(cfg, \"patch_size\", None) if cfg else None\n",
    "    if isinstance(patch_size, (list, tuple)) and len(patch_size) == 2:\n",
    "        ph, pw = patch_size\n",
    "    elif isinstance(patch_size, int):\n",
    "        ph = pw = patch_size\n",
    "    B, C, H, W = pixel_values.shape\n",
    "\n",
    "    if image_size is not None and ph is not None and pw is not None:\n",
    "        if isinstance(image_size, (list, tuple)):\n",
    "            ih, iw = image_size\n",
    "        else:\n",
    "            ih = iw = int(image_size)\n",
    "        gh, gw = max(1, ih // ph), max(1, iw // pw)\n",
    "        if gh * gw == tokens_no_cls:\n",
    "            return gh, gw\n",
    "    if ph is not None and pw is not None and (H % ph == 0) and (W % pw == 0):\n",
    "        gh, gw = H // ph, W // pw\n",
    "        if gh * gw == tokens_no_cls:\n",
    "            return gh, gw\n",
    "    return _infer_grid_hw_from_tokens(tokens_no_cls)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _with_eager_attn_collect_attentions(model, pixel_values):\n",
    "    \"\"\"\n",
    "    Temporarily set attn_implementation='eager' so output_attentions=True is supported.\n",
    "    Restores the previous implementation afterwards.\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    cfg = getattr(model, \"config\", None)\n",
    "    orig_impl = getattr(cfg, \"attn_implementation\", None) if cfg else None\n",
    "\n",
    "    # Prefer official setter if present; else set on config directly.\n",
    "    set_ok = False\n",
    "    if hasattr(model, \"set_attn_implementation\"):\n",
    "        try:\n",
    "            model.set_attn_implementation(\"eager\")\n",
    "            set_ok = True\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not set_ok and cfg is not None:\n",
    "        # Some configs gate via property; set private field directly if needed.\n",
    "        try:\n",
    "            cfg.attn_implementation = \"eager\"\n",
    "            set_ok = True\n",
    "        except Exception:\n",
    "            # last resort for older versions\n",
    "            setattr(cfg, \"_attn_implementation\", \"eager\")\n",
    "            set_ok = True\n",
    "\n",
    "    if not set_ok:\n",
    "        raise RuntimeError(\"Could not switch attention implementation to 'eager'.\")\n",
    "\n",
    "    # Now we can safely request attentions\n",
    "    outputs = model(pixel_values=pixel_values, output_attentions=True, return_dict=True)\n",
    "    atts = outputs.attentions\n",
    "\n",
    "    # restore previous impl\n",
    "    try:\n",
    "        if hasattr(model, \"set_attn_implementation\") and orig_impl is not None:\n",
    "            model.set_attn_implementation(orig_impl)\n",
    "        elif cfg is not None and orig_impl is not None:\n",
    "            cfg.attn_implementation = orig_impl\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return atts\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_attention_heatmap(\n",
    "    model,\n",
    "    pixel_values: torch.Tensor,\n",
    "    layer_idx: int = -1,\n",
    "    head_idx: Optional[int] = None,\n",
    "    rollout: bool = False,\n",
    "    cls_to_patch: bool = True\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Return (B, H', W') heatmaps in [0,1].\"\"\"\n",
    "    atts = _with_eager_attn_collect_attentions(model, pixel_values)  # tuple (L) of (B, heads, T, T)\n",
    "    B, Hh, T, _ = atts[-1].shape\n",
    "    tokens_no_cls = T - 1\n",
    "    grid_h, grid_w = _grid_hw_from_model_and_inputs(model, pixel_values, tokens_no_cls)\n",
    "\n",
    "    if rollout:\n",
    "        eye = torch.eye(T, device=pixel_values.device).unsqueeze(0).expand(B, T, T)\n",
    "        A = eye.clone()\n",
    "        for A_l in atts:\n",
    "            A_mean = A_l.mean(dim=1)\n",
    "            A_hat = A_mean.clamp_min(0) + eye\n",
    "            A_hat = A_hat / A_hat.sum(dim=-1, keepdim=True)\n",
    "            A = torch.bmm(A, A_hat)\n",
    "        cls_row = A[:, 0, 1:] if cls_to_patch else A[:, 1:, 0]\n",
    "        heat = cls_row.reshape(B, grid_h, grid_w)\n",
    "    else:\n",
    "        A_l = atts[layer_idx]\n",
    "        A_sel = A_l.mean(dim=1) if head_idx is None else A_l[:, head_idx]\n",
    "        cls_row = A_sel[:, 0, 1:] if cls_to_patch else A_sel[:, 1:, 0]\n",
    "        heat = cls_row.reshape(B, grid_h, grid_w)\n",
    "\n",
    "    heat = heat - heat.amin(dim=(1,2), keepdim=True)\n",
    "    heat = heat / heat.amax(dim=(1,2), keepdim=True).clamp_min(1e-8)\n",
    "    return heat\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_attention_overlays(\n",
    "    model,\n",
    "    pixel_values: torch.Tensor,\n",
    "    rgb_images: Optional[List] = None,\n",
    "    layer_idx: int = -1,\n",
    "    head_idx: Optional[int] = None,\n",
    "    rollout: bool = False,\n",
    "    cmap: str = \"jet\",\n",
    "    alpha: float = 0.45,\n",
    "    figsize: Union[int, float] = 4,\n",
    "    title_prefix: str = \"Attn\",\n",
    "    save_dir: Optional[str] = None\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    heat = get_attention_heatmap(model, pixel_values, layer_idx, head_idx, rollout)\n",
    "    B, _, H, W = pixel_values.shape\n",
    "    heat_up = F.interpolate(heat.unsqueeze(1), size=(H, W), mode=\"bilinear\", align_corners=False).squeeze(1)\n",
    "\n",
    "    for i in range(B):\n",
    "        plt.figure(figsize=(figsize, figsize))\n",
    "        if rgb_images is not None:\n",
    "            plt.imshow(rgb_images[i])\n",
    "        else:\n",
    "            img = pixel_values[i].detach().cpu()\n",
    "            img0 = (img[0] - img[0].min()) / (img[0].max() - img[0].min() + 1e-8)\n",
    "            plt.imshow(img0, cmap=\"gray\")\n",
    "        plt.imshow(heat_up[i].detach().cpu(), cmap=cmap, alpha=alpha)\n",
    "        plt.title(f\"{title_prefix} | layer={layer_idx} | head={'avg' if head_idx is None else head_idx} | rollout={rollout}\")\n",
    "        plt.axis('off'); plt.tight_layout()\n",
    "\n",
    "        if save_dir is None:\n",
    "            plt.show()\n",
    "        else:\n",
    "            import os\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            fname = f\"attn_layer{layer_idx}_head{'avg' if head_idx is None else head_idx}_rollout{rollout}_idx{i}.png\"\n",
    "            plt.savefig(os.path.join(save_dir, fname), dpi=200, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_batch_from_loader(\n",
    "    model,\n",
    "    data_loader,\n",
    "    device,\n",
    "    n: int = 4,\n",
    "    layer_idx: int = -1,\n",
    "    head_idx: Optional[int] = None,\n",
    "    rollout: bool = False,\n",
    "    mean: List[float] = [0.485, 0.456, 0.406],\n",
    "    std:  List[float] = [0.229, 0.224, 0.225],\n",
    "    save_dir: Optional[str] = None\n",
    "):\n",
    "    model.eval()\n",
    "    batch = next(iter(data_loader))\n",
    "    pixel_values = _extract_pixel_values(batch, device)\n",
    "    if pixel_values.shape[0] > n:\n",
    "        pixel_values = pixel_values[:n]\n",
    "    rgb_images = _denorm_rgb(pixel_values, mean=mean, std=std)\n",
    "    visualize_attention_overlays(\n",
    "        model, pixel_values, rgb_images=rgb_images,\n",
    "        layer_idx=layer_idx, head_idx=head_idx, rollout=rollout,\n",
    "        title_prefix=\"DINOv2 Attention\", save_dir=save_dir\n",
    "    )\n",
    "model.to(device).eval()\n",
    "visualize_batch_from_loader(model, val_loader, device, n=4, layer_idx=-1, head_idx=None, rollout=False)\n",
    "visualize_batch_from_loader(model, val_loader, device, n=4, rollout=True)\n",
    "visualize_batch_from_loader(model, val_loader, device, n=4, layer_idx=-1, head_idx=3, rollout=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
