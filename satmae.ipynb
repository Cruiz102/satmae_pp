{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d91da0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm==0.4.12 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (0.4.12)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.8.0)\n",
      "Requirement already satisfied: opencv-python in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (4.12.0.88)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (3.10.6)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (2.2.6)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (2.3.2)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (0.13.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (0.23.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (1.7.1)\n",
      "Requirement already satisfied: kagglehub in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (0.3.13)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (0.34.4)\n",
      "Requirement already satisfied: albumentations in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (2.0.8)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (4.56.1)\n",
      "Requirement already satisfied: tensorboard in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (2.20.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 10)) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 10)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 10)) (3.6.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from kagglehub->-r requirements.txt (line 11)) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from kagglehub->-r requirements.txt (line 11)) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->-r requirements.txt (line 12)) (1.1.9)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in ./.venv/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 13)) (2.11.7)\n",
      "Requirement already satisfied: albucore==0.0.24 in ./.venv/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 13)) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in ./.venv/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 13)) (4.12.0.88)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 10)) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 10)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 10)) (3.6.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from kagglehub->-r requirements.txt (line 11)) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from kagglehub->-r requirements.txt (line 11)) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->-r requirements.txt (line 12)) (1.1.9)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in ./.venv/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 13)) (2.11.7)\n",
      "Requirement already satisfied: albucore==0.0.24 in ./.venv/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 13)) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in ./.venv/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 13)) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in ./.venv/lib/python3.12/site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 13)) (3.12.6)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in ./.venv/lib/python3.12/site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 13)) (6.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 14)) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 14)) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 14)) (0.6.2)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in ./.venv/lib/python3.12/site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 13)) (3.12.6)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in ./.venv/lib/python3.12/site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 13)) (6.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 14)) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 14)) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 14)) (0.6.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (1.74.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (3.9)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in ./.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (6.32.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (3.1.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (1.74.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (3.9)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in ./.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (6.32.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 15)) (3.1.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 13)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 13)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 13)) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 13)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 13)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 13)) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 15)) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub->-r requirements.txt (line 11)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub->-r requirements.txt (line 11)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub->-r requirements.txt (line 11)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub->-r requirements.txt (line 11)) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 15)) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub->-r requirements.txt (line 11)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub->-r requirements.txt (line 11)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub->-r requirements.txt (line 11)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub->-r requirements.txt (line 11)) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e78d73c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded.\n"
     ]
    }
   ],
   "source": [
    "# Consolidated imports\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import ast\n",
    "import datetime\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import cv2\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import timm\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "from timm.models.vision_transformer import VisionTransformer as TimmVisionTransformer, PatchEmbed\n",
    "\n",
    "assert timm.__version__ >= \"0.3.2\"\n",
    "\n",
    "print(\"Imports loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4e8e5",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc128cca",
   "metadata": {},
   "source": [
    "## lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65e197a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# References:\n",
    "# MAE: https://github.com/facebookresearch/mae\n",
    "# ELECTRA https://github.com/google-research/electra\n",
    "# BEiT: https://github.com/microsoft/unilm/tree/master/beit\n",
    "# --------------------------------------------------------\n",
    "\n",
    "\n",
    "def param_groups_lrd(model, weight_decay=0.05, no_weight_decay_list=[], layer_decay=.75):\n",
    "    \"\"\"\n",
    "    Parameter groups for layer-wise lr decay\n",
    "    Following BEiT: https://github.com/microsoft/unilm/blob/master/beit/optim_factory.py#L58\n",
    "    \"\"\"\n",
    "    param_group_names = {}\n",
    "    param_groups = {}\n",
    "\n",
    "    num_layers = len(model.blocks) + 1\n",
    "\n",
    "    layer_scales = list(layer_decay ** (num_layers - i) for i in range(num_layers + 1))\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "\n",
    "        # no decay: all 1D parameters and model specific ones\n",
    "        if p.ndim == 1 or n in no_weight_decay_list:\n",
    "            g_decay = \"no_decay\"\n",
    "            this_decay = 0.\n",
    "        else:\n",
    "            g_decay = \"decay\"\n",
    "            this_decay = weight_decay\n",
    "            \n",
    "        layer_id = get_layer_id_for_vit(n, num_layers)\n",
    "        group_name = \"layer_%d_%s\" % (layer_id, g_decay)\n",
    "\n",
    "        if group_name not in param_group_names:\n",
    "            this_scale = layer_scales[layer_id]\n",
    "\n",
    "            param_group_names[group_name] = {\n",
    "                \"lr_scale\": this_scale,\n",
    "                \"weight_decay\": this_decay,\n",
    "                \"params\": [],\n",
    "            }\n",
    "            param_groups[group_name] = {\n",
    "                \"lr_scale\": this_scale,\n",
    "                \"weight_decay\": this_decay,\n",
    "                \"params\": [],\n",
    "            }\n",
    "\n",
    "        param_group_names[group_name][\"params\"].append(n)\n",
    "        param_groups[group_name][\"params\"].append(p)\n",
    "\n",
    "    # print(\"parameter groups: \\n%s\" % json.dumps(param_group_names, indent=2))\n",
    "\n",
    "    return list(param_groups.values())\n",
    "\n",
    "\n",
    "def get_layer_id_for_vit(name, num_layers):\n",
    "    \"\"\"\n",
    "    Assign a parameter with its layer id\n",
    "    Following BEiT: https://github.com/microsoft/unilm/blob/master/beit/optim_factory.py#L33\n",
    "    \"\"\"\n",
    "    if name in ['cls_token', 'pos_embed']:\n",
    "        return 0\n",
    "    elif name.startswith('patch_embed'):\n",
    "        return 0\n",
    "    elif name.startswith('blocks'):\n",
    "        return int(name.split('.')[1]) + 1\n",
    "    else:\n",
    "        return num_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8018c",
   "metadata": {},
   "source": [
    "## pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "548dc38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# References:\n",
    "# MAE: https://github.com/facebookresearch/mae\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2D sine-cosine position embedding\n",
    "# References:\n",
    "# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n",
    "# MoCo v3: https://github.com/facebookresearch/moco-v3\n",
    "# --------------------------------------------------------\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid_torch(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = torch.arange(embed_dim // 2, dtype=np.float32, device=pos.device)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = torch.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = torch.sin(out) # (M, D/2)\n",
    "    emb_cos = torch.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = torch.cat([emb_sin, emb_cos], dim=1)  # (M, D)\n",
    "    return emb.double()\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Interpolate position embeddings for high-resolution\n",
    "# References:\n",
    "# DeiT: https://github.com/facebookresearch/deit\n",
    "# --------------------------------------------------------\n",
    "def interpolate_pos_embed(model, checkpoint_model):\n",
    "    if 'pos_embed' in checkpoint_model:\n",
    "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
    "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "        try:\n",
    "            num_patches = model.patch_embed.num_patches\n",
    "        except AttributeError as err:\n",
    "            num_patches = model.patch_embed[0].num_patches\n",
    "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
    "        # height (== width) for the checkpoint position embedding\n",
    "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
    "        # height (== width) for the new position embedding\n",
    "        new_size = int(num_patches ** 0.5)\n",
    "        # class_token and dist_token are kept unchanged\n",
    "        if orig_size != new_size:\n",
    "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
    "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "            # only the position tokens are interpolated\n",
    "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
    "            pos_tokens = torch.nn.functional.interpolate(\n",
    "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
    "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "            checkpoint_model['pos_embed'] = new_pos_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c91180",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19435673",
   "metadata": {},
   "source": [
    "## VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8d05772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# References:\n",
    "# MAE: https://github.com/facebookresearch/mae\n",
    "# timm: https://github.com/rwightman/pytorch-image-models/tree/master/timm\n",
    "# DeiT: https://github.com/facebookresearch/deit\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class VisionTransformer(TimmVisionTransformer):\n",
    "    \"\"\" Vision Transformer with support for global average pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, global_pool=False, **kwargs):\n",
    "        super(VisionTransformer, self).__init__(**kwargs)\n",
    "\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches ** .5),\n",
    "                                            cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        self.global_pool = global_pool\n",
    "        if self.global_pool:\n",
    "            norm_layer = kwargs['norm_layer']\n",
    "            embed_dim = kwargs['embed_dim']\n",
    "            self.fc_norm = norm_layer(embed_dim)\n",
    "\n",
    "            del self.norm  # remove the original norm\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.global_pool:\n",
    "            x = x[:, 1:, :].mean(dim=1)  # global pool without cls token\n",
    "            outcome = self.fc_norm(x)\n",
    "        else:\n",
    "            x = self.norm(x)\n",
    "            outcome = x[:, 0]\n",
    "\n",
    "        return outcome\n",
    "\n",
    "\n",
    "def vit_base_patch16(**kwargs):\n",
    "    model = VisionTransformer(embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "                              norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large_patch16(**kwargs):\n",
    "    model = VisionTransformer(embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "                              norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f4035",
   "metadata": {},
   "source": [
    "## VIT Group Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f962c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# References:\n",
    "# MAE: https://github.com/facebookresearch/mae\n",
    "# timm: https://github.com/rwightman/pytorch-image-models/tree/master/timm\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class GroupChannelsVisionTransformer(TimmVisionTransformer):\n",
    "    \"\"\" Vision Transformer with support for global average pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, global_pool=False, channel_embed=256,\n",
    "                 channel_groups=((0, 1, 2, 6), (3, 4, 5, 7), (8, 9)), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        img_size = kwargs['img_size']\n",
    "        patch_size = kwargs['patch_size']\n",
    "        in_c = kwargs['in_chans']\n",
    "        embed_dim = kwargs['embed_dim']\n",
    "\n",
    "        self.channel_groups = channel_groups\n",
    "\n",
    "        self.patch_embed = nn.ModuleList([PatchEmbed(img_size, patch_size, len(group), embed_dim)\n",
    "                                          for group in channel_groups])\n",
    "        num_patches = self.patch_embed[0].num_patches\n",
    "\n",
    "        # Positional and channel embed\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim - channel_embed))\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(num_patches ** .5), cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        num_groups = len(channel_groups)\n",
    "        self.channel_embed = nn.Parameter(torch.zeros(1, num_groups, channel_embed))\n",
    "        chan_embed = get_1d_sincos_pos_embed_from_grid(self.channel_embed.shape[-1], torch.arange(num_groups).numpy())\n",
    "        self.channel_embed.data.copy_(torch.from_numpy(chan_embed).float().unsqueeze(0))\n",
    "\n",
    "        # Extra embedding for cls to fill embed_dim\n",
    "        self.channel_cls_embed = nn.Parameter(torch.zeros(1, 1, channel_embed))\n",
    "        channel_cls_embed = torch.zeros((1, channel_embed))\n",
    "        self.channel_cls_embed.data.copy_(channel_cls_embed.float().unsqueeze(0))\n",
    "\n",
    "        self.global_pool = global_pool\n",
    "        if self.global_pool:\n",
    "            norm_layer = kwargs['norm_layer']\n",
    "            embed_dim = kwargs['embed_dim']\n",
    "            self.fc_norm = norm_layer(embed_dim)\n",
    "\n",
    "            del self.norm  # remove the original norm\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        x_c_embed = []\n",
    "        for i, group in enumerate(self.channel_groups):\n",
    "            x_c = x[:, group, :, :]\n",
    "            x_c_embed.append(self.patch_embed[i](x_c))  # (N, L, D)\n",
    "\n",
    "        x = torch.stack(x_c_embed, dim=1)  # (N, G, L, D)\n",
    "        _, G, L, D = x.shape\n",
    "\n",
    "        # add channel embed\n",
    "        channel_embed = self.channel_embed.unsqueeze(2)  # (1, c, 1, cD)\n",
    "        pos_embed = self.pos_embed[:, 1:, :].unsqueeze(1)  # (1, 1, L, pD)\n",
    "\n",
    "        # Channel embed same across (x,y) position, and pos embed same across channel (c)\n",
    "        channel_embed = channel_embed.expand(-1, -1, pos_embed.shape[2], -1)  # (1, c, L, cD)\n",
    "        pos_embed = pos_embed.expand(-1, channel_embed.shape[1], -1, -1)  # (1, c, L, pD)\n",
    "        pos_channel = torch.cat((pos_embed, channel_embed), dim=-1)  # (1, c, L, D)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + pos_channel  # (N, G, L, D)\n",
    "        x = x.view(b, -1, D)  # (N, G*L, D)\n",
    "\n",
    "        cls_pos_channel = torch.cat((self.pos_embed[:, :1, :], self.channel_cls_embed), dim=-1)  # (1, 1, D)\n",
    "        cls_tokens = cls_pos_channel + self.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (N, 1 + c*L, D)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.global_pool:\n",
    "            x = x[:, 1:, :].mean(dim=1)  # global pool without cls token\n",
    "            outcome = self.fc_norm(x)\n",
    "        else:\n",
    "            x = self.norm(x)\n",
    "            outcome = x[:, 0]\n",
    "\n",
    "        return outcome\n",
    "\n",
    "\n",
    "def vit_base_patch16(**kwargs):\n",
    "    model = GroupChannelsVisionTransformer(channel_embed=256, embed_dim=768, depth=12,\n",
    "                                           num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "                                           norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large_patch16(**kwargs):\n",
    "    model = GroupChannelsVisionTransformer(channel_embed=256, embed_dim=1024, depth=24,\n",
    "                                           num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "                                           norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f8fda",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b44519d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration created successfully!\n",
      "Key settings:\n",
      "  Model type: vanilla\n",
      "  Model: vit_large_patch16\n",
      "  Batch size: 16\n",
      "  Epochs: 30\n",
      "  Learning rate: 0.0001\n",
      "  Data path: /home/ed/kickoff_pack\n",
      "  Output dir: ./finetune_logs_kaggle\n"
     ]
    }
   ],
   "source": [
    "# Configuration Class for Notebook\n",
    "class Config:\n",
    "    \"\"\"Configuration class to replace argparse for notebook usage\"\"\"\n",
    "    def __init__(self):\n",
    "        # Training parameters\n",
    "        self.batch_size = 16\n",
    "        self.epochs = 30\n",
    "        \n",
    "        # Model parameters\n",
    "        self.model_type = 'vanilla'  # or 'group_c'\n",
    "        self.model = 'vit_large_patch16'\n",
    "        self.input_size = 96\n",
    "        self.patch_size = 8\n",
    "        self.drop_path = 0.2\n",
    "        \n",
    "        # Optimizer parameters\n",
    "        self.clip_grad = None\n",
    "        self.weight_decay = 0.05\n",
    "        self.lr = 1e-4\n",
    "        self.layer_decay = 0.75\n",
    "        self.warmup_epochs = 5\n",
    "        \n",
    "        # Augmentation parameters\n",
    "        self.smoothing = 0\n",
    "        self.mixup = 0.0\n",
    "        self.cutmix = 0.0\n",
    "        \n",
    "        # Finetuning params\n",
    "        self.finetune = None  # Path to pretrained checkpoint\n",
    "        self.global_pool = True\n",
    "        \n",
    "        # Dataset parameters - Updated for current workspace\n",
    "        self.data_path = '/home/ed/kickoff_pack'\n",
    "        self.train_csv = 'data/train.csv'\n",
    "        self.val_csv = 'data/val.csv'\n",
    "        self.images_dir = 'images'\n",
    "        self.kaggle_path = None\n",
    "        \n",
    "        # Model specific\n",
    "        self.grouped_bands = []  # For GroupC vit\n",
    "        self.nb_classes = 10\n",
    "        \n",
    "        # Output and logging\n",
    "        self.output_dir = './finetune_logs_kaggle'\n",
    "        self.log_dir = './finetune_logs_kaggle'\n",
    "        self.device = 'cuda'\n",
    "        self.seed = 0\n",
    "        self.resume = None\n",
    "        self.save_every = 5\n",
    "        \n",
    "        # Runtime parameters\n",
    "        self.start_epoch = 0\n",
    "        self.eval = False\n",
    "        self.num_workers = 4\n",
    "        self.pin_mem = True\n",
    "\n",
    "# Create configuration instance\n",
    "args = Config()\n",
    "\n",
    "print(\"Configuration created successfully!\")\n",
    "print(\"Key settings:\")\n",
    "print(f\"  Model type: {args.model_type}\")\n",
    "print(f\"  Model: {args.model}\")\n",
    "print(f\"  Batch size: {args.batch_size}\")\n",
    "print(f\"  Epochs: {args.epochs}\")\n",
    "print(f\"  Learning rate: {args.lr}\")\n",
    "print(f\"  Data path: {args.data_path}\")\n",
    "print(f\"  Output dir: {args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce3f04f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to run training!\n",
      "You can now call: main(args)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Simplified Single-GPU version of SatMAE++ fine-tuning for Kaggle Dataset\n",
    "# Modified to work with NASA Geographical Objects dataset\n",
    "# --------------------------------------------------------\n",
    "\n",
    "\n",
    "class KaggleGeographicalDataset(Dataset):\n",
    "    \"\"\"Custom dataset for NASA Geographical Objects from Kaggle\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, images_dir, transform=None, target_transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Number of channels (RGB = 3)\n",
    "        self.in_c = 3\n",
    "        \n",
    "        # Filter out missing images during initialization\n",
    "        print(f\"Checking for missing images in {images_dir}...\")\n",
    "        valid_indices = []\n",
    "        missing_count = 0\n",
    "        \n",
    "        for idx, row in self.data.iterrows():\n",
    "            img_name = row['FileName']\n",
    "            img_path = os.path.join(images_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                missing_count += 1\n",
    "                if missing_count <= 5:  # Only print first 5 missing files\n",
    "                    print(f\"Missing image: {img_path}\")\n",
    "                    \n",
    "        if missing_count > 5:\n",
    "            print(f\"... and {missing_count - 5} more missing images\")\n",
    "            \n",
    "        print(f\"Found {len(valid_indices)} valid images out of {len(self.data)} total ({missing_count} missing)\")\n",
    "        \n",
    "        # Keep only rows with existing images\n",
    "        self.data = self.data.iloc[valid_indices].reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path and load image\n",
    "        img_name = self.data.iloc[idx]['FileName']\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        \n",
    "        # Load image using cv2 (BGR format)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get label vector and convert to tensor\n",
    "        label_str = self.data.iloc[idx]['Label Vector']\n",
    "        label_vector = ast.literal_eval(label_str)  # Convert string to list\n",
    "        label = torch.tensor(label_vector, dtype=torch.float32)\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Default preprocessing: resize, normalize, and convert to tensor\n",
    "            image = cv2.resize(image, (96, 96))  # Resize to 96x96 (SatMAE default)\n",
    "            image = image.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1)  # HWC to CHW\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "\n",
    "def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0):\n",
    "    \"\"\"Cosine learning rate scheduler with warmup\"\"\"\n",
    "    warmup_schedule = np.array([])\n",
    "    warmup_iters = warmup_epochs * niter_per_ep\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
    "\n",
    "    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n",
    "    schedule = final_value + (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters))) / 2\n",
    "\n",
    "    schedule = np.concatenate((warmup_schedule, schedule))\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "\n",
    "def train_one_epoch_simple(model, criterion, data_loader, optimizer, device, epoch, \n",
    "                          clip_grad=None, mixup_fn=None, lr_schedule=None):\n",
    "    \"\"\"Simplified training loop for one epoch\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for batch_idx, (samples, targets) in enumerate(data_loader):\n",
    "        # Update learning rate\n",
    "        if lr_schedule is not None:\n",
    "            it = epoch * len(data_loader) + batch_idx\n",
    "            if it < len(lr_schedule):\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_schedule[it]\n",
    "        \n",
    "        samples = samples.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        if mixup_fn is not None:\n",
    "            samples, targets = mixup_fn(samples, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(samples)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_grad is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_samples += samples.size(0)\n",
    "        \n",
    "        # Store predictions and targets for F1 calculation (only if not using mixup)\n",
    "        if mixup_fn is None:\n",
    "            with torch.no_grad():\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                all_predictions.append(preds.cpu().numpy())\n",
    "                all_targets.append(targets.cpu().numpy())\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}/{len(data_loader)}, '\n",
    "                  f'Loss: {loss.item():.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    # Calculate F1 score if we have predictions\n",
    "    if all_predictions and mixup_fn is None:\n",
    "        all_predictions = np.vstack(all_predictions)\n",
    "        all_targets = np.vstack(all_targets)\n",
    "        f1_micro = f1_score(all_targets, all_predictions, average='micro')\n",
    "        f1_macro = f1_score(all_targets, all_predictions, average='macro')\n",
    "    else:\n",
    "        f1_micro = f1_macro = 0.0\n",
    "    \n",
    "    return {'loss': avg_loss, 'f1_micro': f1_micro, 'f1_macro': f1_macro}\n",
    "\n",
    "\n",
    "def evaluate_simple(model, data_loader, device):\n",
    "    \"\"\"Simplified evaluation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for samples, targets in data_loader:\n",
    "            samples = samples.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(samples)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Store predictions\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            all_predictions.append(preds.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "    \n",
    "    f1_micro = f1_score(all_targets, all_predictions, average='micro')\n",
    "    f1_macro = f1_score(all_targets, all_predictions, average='macro')\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    return {'loss': avg_loss, 'f1_micro': f1_micro, 'f1_macro': f1_macro}\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, args, filename=None):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    if filename is None:\n",
    "        filename = f'checkpoint_epoch_{epoch}.pth'\n",
    "    \n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'args': args,\n",
    "    }\n",
    "    \n",
    "    filepath = os.path.join(args.output_dir, filename)\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved: {filepath}\")\n",
    "\n",
    "\n",
    "def find_images_path(args):\n",
    "    \"\"\"Find the correct path for images\"\"\"\n",
    "    possible_paths = [\n",
    "        os.path.join(args.data_path, args.images_dir),\n",
    "    ]\n",
    "    \n",
    "    # If kaggle_path is provided, try that too\n",
    "    if args.kaggle_path:\n",
    "        possible_paths.insert(0, os.path.join(args.kaggle_path, 'images'))\n",
    "    \n",
    "    # Also try common kaggle download locations\n",
    "    home_dir = os.path.expanduser(\"~\")\n",
    "    kaggle_cache = os.path.join(home_dir, '.cache', 'kagglehub')\n",
    "    if os.path.exists(kaggle_cache):\n",
    "        # Look for any directory containing images\n",
    "        for root, dirs, files in os.walk(kaggle_cache):\n",
    "            if 'images' in dirs:\n",
    "                possible_paths.append(os.path.join(root, 'images'))\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            # Check if it actually contains image files\n",
    "            image_files = [f for f in os.listdir(path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if len(image_files) > 0:\n",
    "                print(f\"Found {len(image_files)} images in: {path}\")\n",
    "                return path\n",
    "    \n",
    "    print(f\"Warning: No images found in any of these paths: {possible_paths}\")\n",
    "    return os.path.join(args.data_path, args.images_dir)  # fallback\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    print('Single GPU Training - Kaggle Geographical Dataset')\n",
    "    print('Working directory: {}'.format(os.getcwd()))  # Fixed: Use os.getcwd() instead of __file__\n",
    "    print(\"Configuration:\")\n",
    "    \n",
    "    # Print config in a readable format\n",
    "    config_dict = {k: v for k, v in vars(args).items()}\n",
    "    for key, value in config_dict.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Build datasets\n",
    "    train_csv_path = os.path.join(args.data_path, args.train_csv)\n",
    "    val_csv_path = os.path.join(args.data_path, args.val_csv)\n",
    "    images_path = os.path.join(args.data_path, args.images_dir)\n",
    "    \n",
    "    # Check if images directory exists, if not try alternative locations\n",
    "    if not os.path.exists(images_path):\n",
    "        print(f\"Images directory not found at: {images_path}\")\n",
    "        \n",
    "        # Try some alternative locations\n",
    "        alternative_paths = [\n",
    "            os.path.join(args.data_path, '..', '__MACOSX', 'kickoff_pack', 'images'),\n",
    "            os.path.join(args.data_path, '..', 'images'),\n",
    "            '/tmp/nasa-geographical-objects-multilabel-dataset/images',\n",
    "            # Add kagglehub cache locations\n",
    "            os.path.expanduser('~/.cache/kagglehub/datasets/olebro/nasa-geographical-objects-multilabel-dataset/versions/1/images'),\n",
    "            os.path.expanduser('~/.cache/kagglehub/datasets/olebro/nasa-geographical-objects-multilabel-dataset/versions/2/images'),\n",
    "        ]\n",
    "        \n",
    "        for alt_path in alternative_paths:\n",
    "            if os.path.exists(alt_path):\n",
    "                print(f\"Found images at alternative location: {alt_path}\")\n",
    "                images_path = alt_path\n",
    "                break\n",
    "        else:\n",
    "            print(\"Warning: No images directory found. Please check the data setup.\")\n",
    "            print(\"Available directories:\")\n",
    "            for item in os.listdir(args.data_path):\n",
    "                item_path = os.path.join(args.data_path, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    print(f\"  - {item}/\")\n",
    "    \n",
    "    print(\"Loading datasets...\")\n",
    "    print(f\"Train CSV: {train_csv_path}\")\n",
    "    print(f\"Val CSV: {val_csv_path}\")\n",
    "    print(f\"Images path: {images_path}\")\n",
    "    \n",
    "    dataset_train = KaggleGeographicalDataset(train_csv_path, images_path)\n",
    "    dataset_val = KaggleGeographicalDataset(val_csv_path, images_path)\n",
    "\n",
    "    print(f\"Train dataset size: {len(dataset_train)}\")\n",
    "    print(f\"Val dataset size: {len(dataset_val)}\")\n",
    "\n",
    "    # Setup logging\n",
    "    if args.log_dir is not None and not args.eval:\n",
    "        os.makedirs(args.log_dir, exist_ok=True)\n",
    "        log_writer = SummaryWriter(log_dir=args.log_dir)\n",
    "    else:\n",
    "        log_writer = None\n",
    "\n",
    "    # Data loaders\n",
    "    data_loader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    data_loader_val = DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # Setup mixup (disabled by default for this dataset)\n",
    "    mixup_fn = None\n",
    "    mixup_active = args.mixup > 0 or args.cutmix > 0.\n",
    "    if mixup_active:\n",
    "        print(\"Mixup is activated!\")\n",
    "        mixup_fn = Mixup(\n",
    "            mixup_alpha=args.mixup, cutmix_alpha=args.cutmix,\n",
    "            prob=1.0, switch_prob=0.5, mode='batch',\n",
    "            label_smoothing=args.smoothing, num_classes=args.nb_classes)\n",
    "\n",
    "    # Import the appropriate model module based on model_type\n",
    "    if args.model_type == 'group_c':\n",
    "        print(\"Using SatMAE Group Channel model\")\n",
    "        if len(args.grouped_bands) == 0:\n",
    "            args.grouped_bands = [[0, 1, 2]]  # RGB channels\n",
    "        \n",
    "        print(f\"Grouping bands {args.grouped_bands}\")\n",
    "        # Use the GroupChannelsVisionTransformer we defined earlier\n",
    "        if args.model == 'vit_base_patch16':\n",
    "            model = GroupChannelsVisionTransformer(\n",
    "                patch_size=args.patch_size, img_size=args.input_size, in_chans=dataset_train.in_c,\n",
    "                channel_groups=args.grouped_bands, num_classes=args.nb_classes,\n",
    "                drop_path_rate=args.drop_path, global_pool=args.global_pool,\n",
    "                channel_embed=256, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "                norm_layer=partial(nn.LayerNorm, eps=1e-6)\n",
    "            )\n",
    "        elif args.model == 'vit_large_patch16':\n",
    "            model = GroupChannelsVisionTransformer(\n",
    "                patch_size=args.patch_size, img_size=args.input_size, in_chans=dataset_train.in_c,\n",
    "                channel_groups=args.grouped_bands, num_classes=args.nb_classes,\n",
    "                drop_path_rate=args.drop_path, global_pool=args.global_pool,\n",
    "                channel_embed=256, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "                norm_layer=partial(nn.LayerNorm, eps=1e-6)\n",
    "            )\n",
    "    \n",
    "    elif args.model_type == 'vanilla':\n",
    "        print(\"Using SatMAE Vanilla model\")\n",
    "        # Use the VisionTransformer we defined earlier\n",
    "        if args.model == 'vit_base_patch16':\n",
    "            model = VisionTransformer(\n",
    "                patch_size=args.patch_size, img_size=args.input_size, in_chans=dataset_train.in_c,\n",
    "                num_classes=args.nb_classes, drop_path_rate=args.drop_path, global_pool=args.global_pool,\n",
    "                embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "                norm_layer=partial(nn.LayerNorm, eps=1e-6)\n",
    "            )\n",
    "        elif args.model == 'vit_large_patch16':\n",
    "            model = VisionTransformer(\n",
    "                patch_size=args.patch_size, img_size=args.input_size, in_chans=dataset_train.in_c,\n",
    "                num_classes=args.nb_classes, drop_path_rate=args.drop_path, global_pool=args.global_pool,\n",
    "                embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "                norm_layer=partial(nn.LayerNorm, eps=1e-6)\n",
    "            )\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {args.model_type}\")\n",
    "\n",
    "    # Load pre-trained weights if available\n",
    "    if args.finetune and not args.eval:\n",
    "        if os.path.exists(args.finetune):\n",
    "            checkpoint = torch.load(args.finetune, map_location='cpu', weights_only=False)\n",
    "            print(\"Load pre-trained checkpoint from: %s\" % args.finetune)\n",
    "            \n",
    "            if 'model' in checkpoint:\n",
    "                checkpoint_model = checkpoint['model']\n",
    "            else:\n",
    "                checkpoint_model = checkpoint\n",
    "                \n",
    "            state_dict = model.state_dict()\n",
    "\n",
    "            # Remove incompatible keys for SatMAE models\n",
    "            for k in ['pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'head.weight', 'head.bias']:\n",
    "                if k in checkpoint_model and k in state_dict and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "                    print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                    del checkpoint_model[k]\n",
    "            \n",
    "            # interpolate position embedding\n",
    "            interpolate_pos_embed(model, checkpoint_model)\n",
    "\n",
    "            # Load pre-trained model\n",
    "            msg = model.load_state_dict(checkpoint_model, strict=False)\n",
    "            print(f\"Loading checkpoint: {msg}\")\n",
    "\n",
    "            # manually initialize fc layer for SatMAE models\n",
    "            trunc_normal_(model.head.weight, std=2e-5)\n",
    "        else:\n",
    "            print(f\"Checkpoint file {args.finetune} not found. Training from scratch.\")\n",
    "\n",
    "    model.to(device)\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Model = %s\" % str(model))\n",
    "    print('number of params (M): %.2f' % (n_parameters / 1.e6))\n",
    "\n",
    "    # Build optimizer - Use layer-wise learning rate decay for SatMAE models\n",
    "    param_groups = param_groups_lrd(model, args.weight_decay,\n",
    "                                    no_weight_decay_list=model.no_weight_decay(),\n",
    "                                    layer_decay=args.layer_decay)\n",
    "    optimizer = torch.optim.AdamW(param_groups, lr=args.lr)\n",
    "\n",
    "    # Setup criterion\n",
    "    if mixup_fn is not None:\n",
    "        criterion = SoftTargetCrossEntropy()\n",
    "    elif args.smoothing > 0.:\n",
    "        criterion = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()  # For multi-label classification\n",
    "\n",
    "    print(\"criterion = %s\" % str(criterion))\n",
    "\n",
    "    # Setup learning rate schedule\n",
    "    lr_schedule = cosine_scheduler(\n",
    "        args.lr, 1e-6, args.epochs, len(data_loader_train),\n",
    "        warmup_epochs=args.warmup_epochs, start_warmup_value=1e-6\n",
    "    )\n",
    "\n",
    "    # Resume from checkpoint if specified\n",
    "    start_epoch = args.start_epoch\n",
    "    if args.resume:\n",
    "        if os.path.exists(args.resume):\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu', weights_only=False)\n",
    "            model.load_state_dict(checkpoint['model'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resumed from checkpoint: {args.resume}, epoch {start_epoch}\")\n",
    "\n",
    "    # Evaluation only\n",
    "    if args.eval:\n",
    "        test_stats = evaluate_simple(model, data_loader_val, device)\n",
    "        print(f\"Evaluation on {len(dataset_val)} test images:\")\n",
    "        print(f\"Loss: {test_stats['loss']:.4f}\")\n",
    "        print(f\"F1 Micro: {test_stats['f1_micro']:.4f}\")\n",
    "        print(f\"F1 Macro: {test_stats['f1_macro']:.4f}\")\n",
    "        return\n",
    "\n",
    "    # Training loop\n",
    "    print(f\"Start training for {args.epochs} epochs\")\n",
    "    start_time = time.time()\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        print(f\"\\nEpoch {epoch}/{args.epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        train_stats = train_one_epoch_simple(\n",
    "            model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "            clip_grad=args.clip_grad, mixup_fn=mixup_fn, lr_schedule=lr_schedule\n",
    "        )\n",
    "\n",
    "        # Validation\n",
    "        test_stats = evaluate_simple(model, data_loader_val, device)\n",
    "\n",
    "        print(f\"Training - Loss: {train_stats['loss']:.4f}, F1 Micro: {train_stats['f1_micro']:.4f}, F1 Macro: {train_stats['f1_macro']:.4f}\")\n",
    "        print(f\"Validation - Loss: {test_stats['loss']:.4f}, F1 Micro: {test_stats['f1_micro']:.4f}, F1 Macro: {test_stats['f1_macro']:.4f}\")\n",
    "        \n",
    "        best_f1 = max(best_f1, test_stats[\"f1_micro\"])\n",
    "        print(f'Best F1 Micro so far: {best_f1:.4f}')\n",
    "\n",
    "        # Logging\n",
    "        if log_writer is not None:\n",
    "            log_writer.add_scalar('train/loss', train_stats['loss'], epoch)\n",
    "            log_writer.add_scalar('train/f1_micro', train_stats['f1_micro'], epoch)\n",
    "            log_writer.add_scalar('train/f1_macro', train_stats['f1_macro'], epoch)\n",
    "            log_writer.add_scalar('val/loss', test_stats['loss'], epoch)\n",
    "            log_writer.add_scalar('val/f1_micro', test_stats['f1_micro'], epoch)\n",
    "            log_writer.add_scalar('val/f1_macro', test_stats['f1_macro'], epoch)\n",
    "            log_writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "        # Save checkpoint\n",
    "        if args.output_dir and (epoch % args.save_every == 0 or epoch + 1 == args.epochs):\n",
    "            save_checkpoint(model, optimizer, epoch, args)\n",
    "\n",
    "        # Save logs\n",
    "        log_stats = {\n",
    "            **{f'train_{k}': v for k, v in train_stats.items()},\n",
    "            **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "            'epoch': epoch,\n",
    "            'n_parameters': n_parameters,\n",
    "            'best_f1_micro': best_f1\n",
    "        }\n",
    "\n",
    "        if args.output_dir:\n",
    "            with open(os.path.join(args.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print(f'\\nTraining completed in {total_time_str}')\n",
    "    print(f'Final best F1 Micro: {best_f1:.4f}')\n",
    "\n",
    "    if log_writer is not None:\n",
    "        log_writer.close()\n",
    "\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if args.output_dir:\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Ready to run training!\")\n",
    "print(\"You can now call: main(args)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865368bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single GPU Training - Kaggle Geographical Dataset\n",
      "Working directory: /home/ed/kickoff_pack\n",
      "Configuration:\n",
      "  batch_size: 16\n",
      "  epochs: 30\n",
      "  model_type: vanilla\n",
      "  model: vit_large_patch16\n",
      "  input_size: 96\n",
      "  patch_size: 8\n",
      "  drop_path: 0.2\n",
      "  clip_grad: None\n",
      "  weight_decay: 0.05\n",
      "  lr: 0.0001\n",
      "  layer_decay: 0.75\n",
      "  warmup_epochs: 5\n",
      "  smoothing: 0\n",
      "  mixup: 0.0\n",
      "  cutmix: 0.0\n",
      "  finetune: None\n",
      "  global_pool: True\n",
      "  data_path: /home/ed/kickoff_pack\n",
      "  train_csv: data/train.csv\n",
      "  val_csv: data/val.csv\n",
      "  images_dir: images\n",
      "  kaggle_path: None\n",
      "  grouped_bands: []\n",
      "  nb_classes: 10\n",
      "  output_dir: ./finetune_logs_kaggle\n",
      "  log_dir: ./finetune_logs_kaggle\n",
      "  device: cuda\n",
      "  seed: 0\n",
      "  resume: None\n",
      "  save_every: 5\n",
      "  start_epoch: 0\n",
      "  eval: False\n",
      "  num_workers: 4\n",
      "  pin_mem: True\n",
      "Using device: cuda\n",
      "Images directory not found at: /home/ed/kickoff_pack/images\n",
      "Found images at alternative location: /home/ed/.cache/kagglehub/datasets/olebro/nasa-geographical-objects-multilabel-dataset/versions/1/images\n",
      "Loading datasets...\n",
      "Train CSV: /home/ed/kickoff_pack/data/train.csv\n",
      "Val CSV: /home/ed/kickoff_pack/data/val.csv\n",
      "Images path: /home/ed/.cache/kagglehub/datasets/olebro/nasa-geographical-objects-multilabel-dataset/versions/1/images\n",
      "Checking for missing images in /home/ed/.cache/kagglehub/datasets/olebro/nasa-geographical-objects-multilabel-dataset/versions/1/images...\n",
      "Found 1248 valid images out of 1248 total (0 missing)\n",
      "Checking for missing images in /home/ed/.cache/kagglehub/datasets/olebro/nasa-geographical-objects-multilabel-dataset/versions/1/images...\n",
      "Found 765 valid images out of 765 total (0 missing)\n",
      "Train dataset size: 1248\n",
      "Val dataset size: 765\n",
      "Using SatMAE Vanilla model\n",
      "Images directory not found at: /home/ed/kickoff_pack/images\n",
      "Found images at alternative location: /home/ed/.cache/kagglehub/datasets/olebro/nasa-geographical-objects-multilabel-dataset/versions/1/images\n",
      "Loading datasets...\n",
      "Train CSV: /home/ed/kickoff_pack/data/train.csv\n",
      "Val CSV: /home/ed/kickoff_pack/data/val.csv\n",
      "Images path: /home/ed/.cache/kagglehub/datasets/olebro/nasa-geographical-objects-multilabel-dataset/versions/1/images\n",
      "Checking for missing images in /home/ed/.cache/kagglehub/datasets/olebro/nasa-geographical-objects-multilabel-dataset/versions/1/images...\n",
      "Found 1248 valid images out of 1248 total (0 missing)\n",
      "Checking for missing images in /home/ed/.cache/kagglehub/datasets/olebro/nasa-geographical-objects-multilabel-dataset/versions/1/images...\n",
      "Found 765 valid images out of 765 total (0 missing)\n",
      "Train dataset size: 1248\n",
      "Val dataset size: 765\n",
      "Using SatMAE Vanilla model\n",
      "Model = VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 1024, kernel_size=(8, 8), stride=(8, 8))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (12): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (13): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (14): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (15): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (16): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (17): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (18): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (19): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (20): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (21): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (22): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (23): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_logits): Identity()\n",
      "  (head): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "number of params (M): 302.67\n",
      "criterion = BCEWithLogitsLoss()\n",
      "Start training for 30 epochs\n",
      "\n",
      "Epoch 0/30\n",
      "Model = VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 1024, kernel_size=(8, 8), stride=(8, 8))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (12): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (13): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (14): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (15): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (16): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (17): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (18): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (19): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (20): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (21): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (22): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (23): Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): DropPath()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_logits): Identity()\n",
      "  (head): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "number of params (M): 302.67\n",
      "criterion = BCEWithLogitsLoss()\n",
      "Start training for 30 epochs\n",
      "\n",
      "Epoch 0/30\n",
      "Epoch: 0, Batch: 0/78, Loss: 0.7398, LR: 0.000001\n",
      "Epoch: 0, Batch: 0/78, Loss: 0.7398, LR: 0.000001\n",
      "Epoch: 0, Batch: 50/78, Loss: 0.3326, LR: 0.000014\n",
      "Epoch: 0, Batch: 50/78, Loss: 0.3326, LR: 0.000014\n",
      "Training - Loss: 0.3857, F1 Micro: 0.2421, F1 Macro: 0.1090\n",
      "Validation - Loss: 0.3364, F1 Micro: 0.4221, F1 Macro: 0.1470\n",
      "Best F1 Micro so far: 0.4221\n",
      "Training - Loss: 0.3857, F1 Micro: 0.2421, F1 Macro: 0.1090\n",
      "Validation - Loss: 0.3364, F1 Micro: 0.4221, F1 Macro: 0.1470\n",
      "Best F1 Micro so far: 0.4221\n",
      "Checkpoint saved: ./finetune_logs_kaggle/checkpoint_epoch_0.pth\n",
      "\n",
      "Epoch 1/30\n",
      "Checkpoint saved: ./finetune_logs_kaggle/checkpoint_epoch_0.pth\n",
      "\n",
      "Epoch 1/30\n",
      "Epoch: 1, Batch: 0/78, Loss: 0.2859, LR: 0.000021\n",
      "Epoch: 1, Batch: 0/78, Loss: 0.2859, LR: 0.000021\n",
      "Epoch: 1, Batch: 50/78, Loss: 0.3430, LR: 0.000034\n",
      "Epoch: 1, Batch: 50/78, Loss: 0.3430, LR: 0.000034\n",
      "Training - Loss: 0.3280, F1 Micro: 0.4141, F1 Macro: 0.1538\n",
      "Validation - Loss: 0.3372, F1 Micro: 0.4531, F1 Macro: 0.1865\n",
      "Best F1 Micro so far: 0.4531\n",
      "\n",
      "Epoch 2/30\n",
      "Training - Loss: 0.3280, F1 Micro: 0.4141, F1 Macro: 0.1538\n",
      "Validation - Loss: 0.3372, F1 Micro: 0.4531, F1 Macro: 0.1865\n",
      "Best F1 Micro so far: 0.4531\n",
      "\n",
      "Epoch 2/30\n",
      "Epoch: 2, Batch: 0/78, Loss: 0.3764, LR: 0.000041\n",
      "Epoch: 2, Batch: 0/78, Loss: 0.3764, LR: 0.000041\n",
      "Epoch: 2, Batch: 50/78, Loss: 0.3882, LR: 0.000053\n",
      "Epoch: 2, Batch: 50/78, Loss: 0.3882, LR: 0.000053\n",
      "Training - Loss: 0.3214, F1 Micro: 0.4286, F1 Macro: 0.1773\n",
      "Validation - Loss: 0.3075, F1 Micro: 0.5338, F1 Macro: 0.2577\n",
      "Best F1 Micro so far: 0.5338\n",
      "\n",
      "Epoch 3/30\n",
      "Training - Loss: 0.3214, F1 Micro: 0.4286, F1 Macro: 0.1773\n",
      "Validation - Loss: 0.3075, F1 Micro: 0.5338, F1 Macro: 0.2577\n",
      "Best F1 Micro so far: 0.5338\n",
      "\n",
      "Epoch 3/30\n",
      "Epoch: 3, Batch: 0/78, Loss: 0.2607, LR: 0.000061\n",
      "Epoch: 3, Batch: 0/78, Loss: 0.2607, LR: 0.000061\n",
      "Epoch: 3, Batch: 50/78, Loss: 0.2751, LR: 0.000073\n",
      "Epoch: 3, Batch: 50/78, Loss: 0.2751, LR: 0.000073\n",
      "Training - Loss: 0.3083, F1 Micro: 0.4382, F1 Macro: 0.1758\n",
      "Validation - Loss: 0.3183, F1 Micro: 0.3749, F1 Macro: 0.1329\n",
      "Best F1 Micro so far: 0.5338\n",
      "\n",
      "Epoch 4/30\n",
      "Training - Loss: 0.3083, F1 Micro: 0.4382, F1 Macro: 0.1758\n",
      "Validation - Loss: 0.3183, F1 Micro: 0.3749, F1 Macro: 0.1329\n",
      "Best F1 Micro so far: 0.5338\n",
      "\n",
      "Epoch 4/30\n",
      "Epoch: 4, Batch: 0/78, Loss: 0.2768, LR: 0.000080\n",
      "Epoch: 4, Batch: 0/78, Loss: 0.2768, LR: 0.000080\n"
     ]
    }
   ],
   "source": [
    "# Run the training\n",
    "# Uncomment the line below to start training\n",
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
